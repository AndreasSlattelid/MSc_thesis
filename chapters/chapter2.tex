\chapter{Theoretical Background}
\label{chp_theoretical_background}


\section{Measure Theory}
The measure theory results have been gathered from \cite{lindstrom2017}

\begin{definition}[\textbf{Sigma-algebra}]
Assume that $X$ is a non-empty set, a family $\F$ of subsets of $X$ is called a sigma-algebra if the following holds: 
\begin{enumerate}[label= (\roman*), leftmargin=*]
  \item $\emptyset \in \F$
  \item If $A\in \F$, then $A^{C}\in \F$ 
  \item If $A_{n} \in \F$ for all $n\in \N$, then $\bigcup_{n\in \N}A_{n}\in \F$
\end{enumerate}
\end{definition}

\begin{definition}[\textbf{Measure}]
Assume that $X$ is a non-empty set, and that $\F$ is a $\sigma$-algebra on $X$. 
A measure $\mu$ on $(X,\F)$ is a function $\mu:\F\to \overline{\R}_{+} =[0,\infty)\cup \{\infty\}$
\nomenclature{$\overline{\R}_{+}$}{Positive real line including infinity, i.e $[0,\infty]$}
such that: 
\begin{enumerate}[label = (\roman*), leftmargin=*]
    \item $\mu(\emptyset) = 0$ 
    \item if $\{A_{n}\}_{n\in \N}$ is a pairwise disjoint sequence, then: 
    \[\mu\left(\bigcup_{n\in \N}A_{n}\right) = \sum_{n\in \N}\mu(A_{n})\]
\end{enumerate}
We call the triplet $(X,\F,\mu)$ a measure space.
\end{definition}

\begin{proposition}[\textbf{Intersection of $\sigma$-algebras is a $\sigma$-algebra}]
Let $(X,\F, \mu)$ be a measure space, let $\mathcal{I}$ be a non-empty index set and let $\G_{i}$, $i\in \mathcal{I}$ be $\sigma$-algebras on $X$, then: 
\[\G = \bigcap_{i \in \mathcal{I}}\G_{i} = \{A\subseteq X: A\in \G_{i},\; \forall\; i\in \mathcal{I}\}
\] 
is a $\sigma$-algebra on $X$
\end{proposition}

\begin{proof}
Since all $\G_{i}$'s are $\sigma$-algebras, we have that $\emptyset \in \G_{i}\; \forall \; i \in \mathcal{I}$, thus $\emptyset \in \G$. 
\\~\\ 
Assume that $A \in \G$, meaning that $A \in \G_{i}\;\forall i \in \mathcal{I}$, now: since all $\G_{i}$'s are $\sigma$-algebras we have that $A^{C} \in \G$
\\~\\
Assume that $\{A_{n}\}_{n\in \N} \in \G$, then we have that $\{A_{n}\}_{n\in \N} \in \G_{i}\;\forall i\in \mathcal{I}$, and thus: $\bigcup_{n\in \N}A_{n} \in \G$.
\end{proof}


\begin{proposition}[\textbf{Continuity of measure}]
Let $\{A_{n}\}_{n\in \N}$ be a sequence of measurable sets in $(X, \F, \mu)$, then we have: 
\begin{enumerate}[label = (\roman*), leftmargin=*]
    \item Assume that $\{A_{n}\}_{n\in \N}$ is an increasing sequence, i.e that $A_{n} \subseteq A_{n+1}$ for all $n\in \N$, then: 
    \[\mu\left(\bigcup_{n\in \N}A_{n}\right) = \lim_{n\to \infty}\mu(A_{n})
    \]
    \item Assume that $\{A_{n}\}_{n\in \N}$ is a decreasing sequence, i.e that $A_{n+1} \subseteq A_{n}$ for all $n\in \N$, and that $\mu(A_{1}) < \infty$ then: 
    \[\mu\left(\bigcap_{n\in \N}A_{n}\right) = \lim_{n\to \infty}\mu(A_{n})
    \]
\end{enumerate}
\end{proposition}

\begin{definition}[\textbf{Null set}]
A set $N\subseteq X$ is called a null set, if there is a set $B\in \F$ such that $N\subseteq B$ and $\mu(B) = 0$. 
\end{definition}

\begin{definition}[\textbf{Complete measure space}]
A measure space $(X,\F, \mu)$ is called complete if all null sets belong to $\F$.
\end{definition}

Let $\mathcal{N}$ denote the collection of all null sets.

\begin{theorem}[\textbf{Complete measure space with complete measure}]
Assume that $(X, \F, \mu)$ is a measure space, and let:\\
$\overline{\F} = \{A\cup N: A\in \F \; and\; N\in \mathcal{N} \}$, define $\overline{\mu}:\overline{\F} \to \overline{\R}_{+}$ by: 
\[\overline{\mu}(A\cup N) = \mu(A), \;\; \forall A \in \F
\]
Then $(X,\overline{\F}, \overline{\mu})$ is a complete measure space extending $(X,\F,\mu)$. 
\end{theorem}

\begin{proposition}
Let $X$ be a nonempty set, and $\mathcal{A}$ a collection of subsets of $X$. Then there exists a smallest $\sigma$-algebra $\sigma(\mathcal{A})$ containing $\mathcal{A}$. Such that if $\mathcal{C}$ is any other $\sigma$-algebra containing $\mathcal{A}$ then $\sigma(\mathcal{A}) \subseteq \mathcal{C}$. 
\end{proposition}

\begin{definition}[\textbf{Borel $\sigma$-algebra}]
We define the Borel $\sigma$-algebra $\mathcal{B}$ as the smallest $\sigma$-algebra generated by all open sets on $\R$.
\end{definition}

\begin{example}[\textbf{Lesbegue Measure}]
Let $X = \R$ and $\mathcal{B}$ the Borel $\sigma$-algebra, the Lesbegue measure is a measure $\mu$ such that: 
\[
\mu([a,b]) = b-a
\]
\end{example}



\centerline{\textbf{Measurable functions}}
\begin{definition}[\textbf{Inverse image of $B$ under $f$}]
Let $X,Y$ be two non-empty sets, and let $f:X\to Y$ with $B\subseteq Y$, we then define the inverse image of $B$ as: 
\[f^{-1}(B) = \{x\in X: f(x)\in B\}
\]
\end{definition} 

We use the convention that $\overline{\R} = \R\cup \{-\infty, \infty\}$ \nomenclature{$\overline{\R}$}{Extended real-line i.e $[-\infty, \infty]$}


\begin{definition}[\textbf{Measurable function}]
Let $(X, \F, \mu)$ be a measure space. A function $f:X \to \overline{\R}$ is measurable if:
\begin{align*}
f^{-1}([-\infty, r)) \in \F   
\end{align*}
\end{definition}

\begin{proposition}
Assume that $f,g:X\to \R$ are measurable functions, then: 
\begin{enumerate}[label = (\roman*), leftmargin=*]
    \item $f+g$ is measurable.
    \item $f-g$ is measurable. 
    \item $fg$  is measurable.
\end{enumerate}
\end{proposition}


\centerline{\textbf{Integration of non negative functions}}

\begin{definition}[\textbf{Integration of simple function}]
\label{def: int_simple_function}
Assume that: 
\[
f(x) = \sum_{i=1}^{n}a_{i}\mathbbm{1}_{A_{i}}(x)
\]
is a non negative simple function on standard form i.e. $X = \bigcup_{i=1}^{n} A_{i}$ with $A_{i} = \{x\in X: f(x) = a_{i}\}$ disjoint and measurable. The integral of $f$ with respect to $\mu$ is defined as: 
\begin{align*}
\int fd\mu &= \sum_{i=1}^{n}a_{i}\mu(A_{i})    
\end{align*}
We use the convention that $0\cdot \infty = 0$
\end{definition}

\begin{definition}
If $f:X \to \overline{\R}_{+}$ is measurable we define: 
\begin{align*}
\int f d\mu 
&= 
\sup\left\{
\int gd\mu: \text{$g$ is a non negative simple function, $g\leq f$}
\right\}
\end{align*}
\end{definition}

\begin{proposition}
\label{prop: simple_functions_conv_pointwise_to_f}
If $f: X \to \overline{\R}_{+}$ is a measurable function, there exists an increasing sequence $\{h_{n}\}$ of simple functions converging pointwise to $f$. Moreover, for each $n$ and each $x\in X$ either:
\[
f(x) - \frac{1}{2^{n}} < h_{n}(x) \leq f(x) \;\;\text{or}\;\;
h_{n}(x) = 2^{n}
\]
\end{proposition}

\begin{theorem}[\textbf{Monotone Convergence Theorem}]
Assume that $f:X \to \overline{\R}_{+}$ is measurable, and assume that $\{f_{n}\}$ is an increasing sequence of non-negative measurable functions converging pointwise to $f$ so that $\lim\limits_{n\to \infty}f_{n}(x) = f,\; \forall x\in X$, then: 
\[ \lim_{n\to \infty}\int f_{n}d\mu = \int \lim_{n\to \infty}f_{n}d\mu
\]
\end{theorem}

\begin{theorem}[\textbf{Fatou's lemma}]
Let $\{f_{n}\}$ be a sequence of non-negative measurable functions, then: 
\[\liminf_{n\to \infty}\int f_{n}d\mu \geq \int \liminf_{n\to \infty}f_{n}d\mu
\]
\end{theorem}

\begin{definition}
A function $f:X \to \overline{\R}_{+}$ is said to be integrable if it is measurable and $\int f d\mu < \infty$    
\end{definition}


\newpage 
\centerline{\textbf{Integration of general functions}}
We would also like to integrate functions taking negative values as well, we then observe that if $f:X \to \overline{\R}$ then $f = f_{+} - f_{-}$ with: 
\begin{align*}
f_{+}(x) &= \begin{cases}
      f(x) & \text{$f(x) \geq 0$}\\
      0 & \text{otherwise}
    \end{cases} 
\;\;\;    
f_{-}(x) = \begin{cases}
      -f(x) & \text{$f(x) < 0$}\\
      0 & \text{otherwise}
    \end{cases}      
\end{align*}

\begin{definition}
A function $f: X \to \overline{\R}$ is called integrable it if is measurable, and $f_{+}$ and $f_{-}$ are integrable, we define the integral of $f$ as: 
\begin{align*}
\int fd\mu &= \int f_{+}d\mu - \int f_{-}d\mu    
\end{align*}
\end{definition} 

\begin{lemma}
A measurable function $f$ is integrable if and only if it's absolute value $|f|$ is integrable i.e. if and only if: 
\begin{align*}
\int |f| d\mu < \infty    
\end{align*}
\end{lemma}

\begin{theorem}[\textbf{Lebesgue's Dominated Convergence Theorem}]
Assume that $g:X \to \overline{\R}_{+}$ is a non-negative, integrable function and that $\{f_{n}\}$ is a sequence of measurable functions converging pointwise to $f$. 
If $|f_{n}| \leq g$ for all $n$, then: 
\begin{align*}
\lim\limits_{n \to \infty}\int f_{n}d\mu &= \int fd\mu    
\end{align*}
\end{theorem} 

\centerline{\textbf{Riemann and Lesbegue integration}}

\begin{theorem}
Assume that $f:[a,b]\to [0,\infty)$ is a bounded Riemann integrable function on $[a,b]$. Then $f$ is measurable and the Riemann and Lebesgue integral coincide:
\[
\int_{a}^{b}f(x)dx = \int_{[a,b]}fd\mu
\]
\end{theorem}

\centerline{\textbf{$L^{p}$-spaces}}

\begin{definition}{\textbf{$\mathcal{L}^{p}$}}
If $1 \leq p < \infty$ and $(X,\F, \mu)$ is a measure space, we define: 
\begin{align*}
\mathcal{L}^{p}(X,\F, \mu) = 
\{
f:X \to \overline{C}: f\:\text{is measurable and}
\int |f|^{p}d\mu < \infty
\}
\end{align*}
furthermore, define: 
\begin{align*}
\norm{f}_{p} = \left(\int |f|^{p}d\mu \right)^{\frac{1}{p}}    
\end{align*}
\end{definition}

\begin{definition}[$L^{p}$]
Let $p \in [1,\infty)$, and define a relation by: 
\[
f \sim g \iff f = g\;\; a.e.
\] 
Consider the equivalence class:
\[
[f]:= \{g \in \mathcal{L}^{p}: g \sim f\}
\]
We then define: 
\begin{align*}
L^{p}(X,\F, \mu) := \{[f]: f\in \mathcal{L}^{p}\}    
\end{align*}
\end{definition}











\newpage 

\section{Probability theory}

Most of the results in this section are gathered from \cite{walsh2012knowing}, let $\F$ be a $\sigma$-algebra and $\Omega$ let denote the sample space. 

\begin{definition}[\textbf{Probability measure}]
A probability measure $P$ on $(\Omega, \F)$ is a function $P:\F \to [0,1]$ such that: 
\begin{enumerate}[label = (\roman*), , leftmargin=*]
    \item if $A \in \F $, then $P(A)\geq 0$
    \item $P(\Omega) = 1$
    \item if $\{A_{n}\}_{n\in \N}$ is a pairwise disjoint sequence, then: 
    \[P\left(\bigcup_{n\in \N}A_{n}\right) = \sum_{n\in \N}P(A_{n})\]
\end{enumerate}
\end{definition}

\begin{definition}[\textbf{Random Variable}]
Let $(\Omega, \F, P)$ be a probability space. A random variable $X$ is a function $X:\Omega \to \R$ such that: 
\begin{align*}
\{\omega: X(\omega) \leq x\} \in \F   
\end{align*}
\end{definition}

\begin{proposition}
Let $X$ be a random variable, and let $A \in \mathcal{B}$ (the Borel $\sigma$-algebra), then: 
\begin{align*}
\{X \in A\} \in \F    
\end{align*}
\end{proposition}

\centerline{\textbf{Expectations and Conditional Expectations}}
We will now use the results from measure theory and see how it relates to the construction of expectations and conditional expectations. 

\begin{definition}[\textbf{Discrete random variable}]
We say that a random variable $X$ is discrete if: 
\[
X(\omega) = \sum_{i=1}^{\infty}x_{i}\mathbbm{1}_{A_{i}}(\omega)
\]
Where $A_{i} = \{X = x_{i}\}$, furthermore we assume that it is on standard-form (See Definition \ref{def: int_simple_function})
\end{definition}

\begin{definition}[\textbf{Expectation Discrete case}]
Let $X$ be a discrete random variable, we say that $X$ is integrable if:
\[
\sum_{i=1}^{\infty}|x_{i}|P(A_{i}) < \infty
\]
If $X$ is integrable, we define the expectation as:
\[
\E[X] = \sum_{i=1}^{\infty}x_{i}P(A_{i})
\]
\end{definition}

In order to define the expectation of a general random variable $X$ one also consider sequences of non-negative simple functions, and decomposes the expectation in two positive random variables, i.e. $X = X^{+} - X_{-}$ whit $X^{+} = \max(X,0)$ and $X^{-} = \max(-X,0)$, and define the expectation as: 
\[
\E[X] = \E[X^{+}] - \E[X^{-}]
\]

We will mostly consider the expectation as a measure-theoretic integral, i.e: 
\[
\E[X] = \int_{\Omega}X(\omega)dP(\omega)
\]


\begin{definition}[\textbf{Conditional Expectation}]
Let $(\Omega, \F, P)$ be a probability space, let $X$ be an integrable random variable, and let $\mathcal{G} \subseteq \F$ be a sub $\sigma$-algebra, we say that a random variable $Z = \E[X|\mathcal{G}]$ is the conditional expectation of $X$ given $\mathcal{G}$ if: 
\begin{enumerate}[label= (\roman*), , leftmargin=*]
    \item $Z$ is $\mathcal{G}$-measurable, and
    \item if $A\in \mathcal{G}$, then:
    \[
    \int_{A}ZdP = \int_{A}XdP
    \]
\end{enumerate}
\end{definition}

\begin{theorem}
\label{thm: Conditional_expectation_rules}
Let $X$ and $Y$ be integrable random variables, let $a,b \in \R$ and let $\mathcal{G} \subseteq \F$ be a sub $\sigma$-algebra, then: 
\begin{enumerate}[label= (\roman*), , leftmargin=*]
    \item $\G = \{\emptyset, \Omega\}$, then: $\E[X|\G] = \E[X]$
    \item $\E[\E[X|\G]] = \E[X]$
    \item If $X$ is $\G$-measurable, $\E[X|\G] = X$ a.e.
    \item $\E[aX+bY|\G] = a\E[X|\G] + b\E[Y|\G]$ a.e. 
    \item If $X\geq 0$ a.e., $\E[X|\G] \geq 0$ a.e. 
    \item If $X\leq Y$ a.e., $\E[X|\G] \leq \E[Y|\G]$ a.e. 
    \item $|\E[X|\G]| \leq \E[|X||\G]$ a.e. 
    \item Suppose that $Y$ is $\G$-measurable and $XY$ are integrable, then: 
    \[
    \E[XY|\G] = Y\E[X|\G]\;\;a.e.
    \]
    \item If $X$ and $\G$ are independent, then: 
    \[
    \E[X|\G] = \E[X]\;\; a.e.
    \]
    \item If $X_{n}$ and $X$ are integrable, and either $X_{n} \uparrow X$, or $X_{n} \downarrow X$, then: 
    \[
    \E[X_{n}|\G] \to \E[X|\G]\;\; a.e.
    \]
\end{enumerate}
\end{theorem}

\begin{theorem}[\textbf{Tower Law}]
\label{thm: Tower_law}
If $X$ is an integrable random variable, and if $\G_{1} \subseteq \G_{2}$ are $\sigma$-algebras, then: 
\[
\E[\E[X|\G_{1}]|\G_{2}] = \E[\E[X|\G_{2}]|\G_{1}] = \E[X|\G_{1}]
\]
\end{theorem}

\begin{theorem}[\textbf{Jensen's inequality}]
\label{thm: Jensen's_ineuality}
Let $\phi$ be a convex function on an open interval $(x_{1}, x_{2})$ and let $X$ be a random variable whose range is in $(x_{1}, x_{2})$. Suppose $X$ and $\phi(X)$ are integrable and that $\G \subseteq \F$ are $\sigma$-algebras, then: 
\[
\phi\left(\E[X|\G]\right) \leq \E\left[\phi(X)|\G \right]\;\;a.e.
\]
\end{theorem}

\newpage 
\section{Stochastic Analysis}
The results in this section are based on \cite{walsh2012knowing} and \cite{baldi2017stochastic}. 
\\~\\
\centerline{\textbf{Stochastic processes and filtrations}}

\begin{definition}[\textbf{Filtration}]
Let $\T$ denote an index set either countable or a subset of $\R$, we say that the collection $\mathbbm{F} = (\F_{t})_{t \in \T}$ of $\sigma$-algebras is a filtration if for every $s\leq t \in \T$: 
\[
\F_{s}\subseteq \F_{t}
\]
\end{definition}

\begin{definition}[\textbf{Augmented Filtration}]
The augmented filtration is the filtration obtained by including the collection of null sets $\mathcal{N}$ to the $\sigma$-algebra $\F_{t} = \sigma(X_{u}:u\leq t)$, i.e:
\[
\overline{\F}_{t} = \sigma(\F_{t}\cup \mathcal{N})
\]
\end{definition}

\begin{definition}[\textbf{Stochastic process}]
Let $(\Omega, \F, (\F_{t})_{t\in \T}, P)$ denote a probability equipped with a filtration $(\F_{t})_{t\in \T}$.\\ 
A stochastic process $X = (X_{t})_{t\in \T}$ is a collection of random variables defined on $(\Omega, \F)$ taking values in a measurable space $(E, \mathcal{E})$    
\end{definition}

\begin{definition}[\textbf{Adapted process}]
We say that the stochastic process $X = (X_{t})_{t\in \T}$ is adapted to the filtration $\mathbbm{F} = (\F_{t})_{t \in \T}$ if for every $t\in \T$ we have that $X_{t}$ is $\F_{t}$-measurable. 
\end{definition}

\begin{definition}[\textbf{Modification and Indistinguishable processes}]
Let $(\Omega, \F, (\F_{t})_{t\geq 0}, P) = (\Omega', \F',(\F_{t})_{t\geq 0}, P')$, we say that $X$ is a modification of $X'$ if:
\[
\forall\; t \;\; P(X_{t} = X_{t}') = 1
\]
We say that $X$ is indistinguishable from $X'$ if:
\[
P(X_{t} = X_{t}'\;\forall\; t) = 1
\]
\end{definition}

\begin{definition}[\textbf{$\sigma$-finite measure \cite{lindstrom2017}}]
We say that a measure space $(X, \F, \mu)$ is $\sigma$-finite if $X$ is a countable union of sets with finite measure, i.e for $\{A_{n}\}_{n\in \N} \in \F$ we have: 
\begin{align*}
X = \bigcup_{n\in \N}A_{n}\;\;\text{with}\;\;\mu(A_{n}) < \infty,\; \forall n \in \N    
\end{align*}
\end{definition}

\begin{theorem}[\textbf{\cite{lindstrom2017}}]
Assume that $(X, \F, \mu)$ and $(Y, \G, \nu)$ are two measure spaces, and let $\F\otimes \G$ denote the $\sigma$-algebra generated by the measurable rectangles $F\times G, F \in \F, G\in \G$. Then there exists a measure $\mu\times \nu$ on $(\F\otimes \G)$ such that: 
\[
\mu\times\nu(F\times G) = \mu(F)\nu(G)\;\;\text{for all $F\in \F, G \in \G$}
\]
If $\mu$ and $\nu$ are $\sigma$-finite, this measure is unique and is called the product measure of $\mu$ and $\nu$. 
\end{theorem}

\begin{definition}[\textbf{Measurable Process}]
A stochastic process $X = (X_{t})_{t\geq 0}$ taking values on a measurable space $(E, \mathcal{E})$ is said to be measurable if:
\[
A\times \Omega \ni (t,\omega) \mapsto X_{t}(\omega) \in E
\]
is measurable (with $A\subseteq E$), i.e:
\begin{align*}
\forall B \in \mathcal{B}(E): \;\; 
\{
(t,\omega) \in A \times \Omega: X_{t}(\omega) \in B
\} \in \mathcal{B}(A) \otimes \F
\end{align*}
\end{definition}

\begin{definition}[\textbf{Progressively measurable process}]
A stochastic process $X = (X_{t})_{t\geq 0}$ is said to be progressively measurable w.r.t $\mathbbm{\F}$ if:
\begin{align*}
\forall t:\;\; 
[0,t]\times \Omega \ni (s,\omega) \mapsto X_{s}(\omega)
\end{align*}
is measurable w.r.t $\mathcal{B}([0,t])\otimes \F_{t}$
\end{definition}


\begin{theorem}[\textbf{Kolmogorov's continuity theorem}]
Let $D\subseteq \R^{m}$, be an open set, and consider the process $X = (X_{\theta})_{\theta \in D}$ and assume there exists $\alpha >0, \beta>0, C>0$ such that:
\[E[|X_{\theta_{1}} - X_{\theta_{2}}|^{\beta}] \leq C|\theta_{1}-\theta_{2}|^{m + \alpha}
\]
then there exists a continuous modification $\widetilde{X}$ of $X$. Furthermore $\widetilde{X}$ is Hölder continuous with exponent $\gamma < \frac{\alpha}{\beta}$ on all compact subsets $K\subseteq D$, i.e: 
\[|\widetilde{X}_{\theta_{1}} -\widetilde{X}_{\theta_{2}}| \leq C|\theta_{1}-\theta_{2}|^{\gamma}
\]
\end{theorem}

\centerline{\textbf{Integral Spaces}}

\begin{definition}[$M^{p}_{loc}$]
Let $M_{loc}^{p}([a,b])$ denote the space of equivalence classes of real-valued progressively measurable processes $X = (X_{t})_{t\geq 0} \in \R^{d}$ such that: 
\begin{align*}
\int_{0}^{\infty}|X_{s}|^{p}ds < \infty \;\; \text{a.s}    
\end{align*}
\end{definition}

\begin{definition}[$M^{p}$]
Let $M^{p}[a,b]$ denote the subspace of $M^{p}_{loc}[a,b]$ such that: 
\begin{align*}
\E\left[
\int_{a}^{b}|X_{s}|^{p}ds
\right] < \infty   
\end{align*}
\end{definition}

\newpage 
\centerline{\textbf{Fubini and Stochastic Fubini}}
\label{thm: Fubini}
\begin{theorem}[\textbf{Fubini's theorem}]
Let $(X, \F, \mu)$ and $(Y, \G, \nu)$ be two $\sigma$-finite measure spaces, and assume that $f:X\times Y \to \overline{\R}$ is $\mu\times\nu$-integrable, i.e. 
\[
\iint |f(x,y)|d(\mu\times \nu) < \infty
\]
Then: 
\begin{align*}
x \mapsto \int f(x,y)d\nu(y) \;\;\text{and}\;\; y\mapsto \int f(x,y)d\mu(x)    
\end{align*}
are $\mu-$ and $\nu-$integrable, respectively. Moreover:
\begin{align*}
\int_{X\times Y} f d(\mu \times \nu) &=     
\int_{X} \left[
\int_{Y} f(x,y)d\nu(y)
\right]d\mu(x) 
= 
\int_{Y} \left[
\int_{X} f(x,y)d\mu(x)
\right]d\nu(y) 
\end{align*}

The functions $y \mapsto \int f(x,y)d\mu(x)$ and $x \mapsto \int f(x,y)d\nu(y)$ are defined $\mu$ (a.e.) and $\nu$ (a.e.) respectively. 
\end{theorem} 

\begin{theorem}[\textbf{Stochastic Fubini for Brownian Motion \cite{filipovic2009term}}]
\label{thm: Stochastic_Fubini}
Let $X = (X(\omega, t, s))_{[0\leq t,s\leq T]}$ be an $\R^{d}$-valued stochastic process satisfying: 
\begin{itemize}[leftmargin=*]
    \item $X$ is progressively measurable w.r.t $\F_{T}\otimes \mathcal{B}([0,T])$ 
    \item $\sup\limits_{0 \leq s,t\leq T}|{X(t,s)}| < \infty$
\end{itemize}
Then $\lambda(t) = \int_{0}^{T}X(t,s)ds \in M^{2}_{loc}[0,T]$ and there exists a $\F_{T}\otimes \mathcal{B}([0,T])$-measurable modification $\psi(s)$of $\int_{0}^{T}X(t,s)ds $ such that $\psi \in M^{2}_{loc}([0,T])$, moreover: 
\begin{align*}
\int_{0}^{T}\psi(s)ds &= \int_{0}^{T}\lambda(t)dW(t)    
\end{align*}
i.e. 
\begin{align*}
\int_{0}^{T}\left[
\int_{0}^{T}X(t,s)dW(t)
\right]ds 
&= 
\int_{0}^{T}\left[
\int_{0}^{T}X(t,s)ds
\right]dW(t) 
\end{align*}
\end{theorem}

\newpage 
\centerline{\textbf{Girsanov's theorem, Equivalent martingale measures and Bayes theorem}}
\text{}\newline
Let $W = (W_{t})_{t\in [0,T]} \in \R^{m}$ denote a Brownian motion on $(\Omega, \F, (\F_{t})_{t\in [0,T]}, P)$, furthermore let $\phi$ be an $\R^{m}$-valued process (also valid for $\C^{m}$) with \\
$\phi \in M^{2}_{loc}([0,T])$, the process we will be interested in looks like: 
\begin{align}
\label{eq: Dodeans_exponential}
Z_{t} := \mathcal{E}_{t}\left(
\phi \bullet W
\right) =
\exp\left(
\int_{0}^{t}\phi_{s}dW_{s} - \frac{1}{2}\int_{0}^{t}\phi_{s}^{2}ds
\right)
\end{align}

\begin{definition}[\textbf{Absolutely continuous probability measures}]
\label{def: absolutely_cont_measures}
Let $P$ and $Q$ be two probability measures on $(\Omega, \F)$, and define: 
\begin{align*}
\mathcal{N}_{P} &= \{A \in \F: P(A) = 0\} \\ 
\mathcal{N}_{Q} &= \{A \in \F: Q(A) = 0\} 
\end{align*} 
We say that $Q$ is absolutely continuous w.r.t $P$ iff $\mathcal{N}_{P} \subseteq \mathcal{N}_{Q}$ and we write $Q \ll P$, i.e $P(A) = 0 \implies Q(A)$
\end{definition}

\begin{definition}[\textbf{Equivalent probability measures}]
Consider the situation described in definition \ref{def: absolutely_cont_measures}, we say that $Q$ and $P$ are equivalent iff $\mathcal{N}_{P} \subseteq \mathcal{N}_{Q}$ and 
$\mathcal{N}_{Q} \subseteq \mathcal{N}_{P}$ i.e.  $\mathcal{N}_{P}= \mathcal{N}_{Q}$
and we write $Q\sim P$, i.e: 
$$
P(A) = 0 \iff Q(A) = 0
$$
\end{definition}

\begin{theorem}[\textbf{Radon Nikodym derivative}]
Let $Q\sim P$ and define $Q(A) = \int_{A}Z_{T}dP$, where $A \in \F$ and $Z$ is defined as in equation \ref{eq: Dodeans_exponential}, furthermore require that $\E[Z_{T}] = 1$, then $Q$ defines a new probability measure on $(\Omega, \F)$, and $$
\frac{dQ}{dP}\bigg{|}_{\F_{T}} = Z_{T}
$$     
\end{theorem} 

\begin{theorem}[\textbf{Girsanov's theorem \cite{baldi2017stochastic}}]
\label{thm: Girsanov's_thm}
Let $W = (W_{t})_{t\in [0,T]}$ be an $m$-dimensional Brownian motion on $(\Omega, \F, (\F_{t})_{t\in [0,T]}, P)$, let $Z = (Z_{t})_{t\in [0,T]}$  be defined as in equation \ref{eq: Dodeans_exponential} with $\phi \in M^{2}_{loc}[0,T]$. Furthermore assume that $Z$ is a martingale w.r.t $P$ and let $Q$ be a probability measure on $(\Omega, \F)$ defined via the Radon-Nikodym density $Z_{T}$, then: 
\begin{align*}
W_{t}^{Q} &= W_{t} - \int_{0}^{t}\phi_{s}ds    
\end{align*}
defines a $(Q,\F)$-Brownian motion on $[0,T]$
\end{theorem}

Often what makes Girsanov's theorem hard to use is the requirement of $Z$ being a martingale under $P$, therefore the next theorem is quite useful: 

\newpage 
\begin{theorem}[\textbf{\cite{baldi2017stochastic}}]
\label{thm: Novikov_cond_and_implications}
Let $\phi \in M^{2}_{loc}([0,T])$, define $M_{t} = \int_{0}^{t}\phi_{s}dW_{s}, t \in [0,T]$ with $\langle M \rangle_{t} = \int_{0}^{t}\phi^{2}_{s}ds$, and let: 
$$
Z_{t} = \exp\left(
M_{t} - \frac{1}{2}\langle M \rangle_{t}
\right)
$$
Consider the following properties: 
\begin{enumerate}[label= (\roman*), leftmargin=*]
    \item $\E\left[e^{\frac{1}{2}\int_{0}^{T}|\phi_{s}|^{2}ds}\right] < \infty$ (Novikov's condition)
    \item $M = (M_{t})_{t\in [0,T]}$ is a bounded martingale in $L^{2}(\Omega, \F, P)$ and \\ 
    $\E[e^{\frac{1}{2} M_{T}}] < \infty$ 
    \item $Z = (Z_{t})_{t\in [0,T]}$ is a uniformly integrable martingale. 
\end{enumerate}

Then $(i) \implies (ii) \implies (iii)$
\end{theorem} 

\begin{theorem}[\textbf{Bayes theorem \cite{oksendal2003stochastic}}]
\label{thm: Bayes_thm}
Let $P$ and $Q$ be two probability measures on $(\Omega, \F)$ such that $\frac{dQ}{dP} = f$ with $f\in L^{1}(\Omega, \F, P)$. Let $X$ be a random variable on $(\Omega, \F)$ such that: 
\begin{align*}
\E_{Q}[|X|] &= \int_{\Omega}|X(\omega)|f(\omega)dP(\omega) < \infty
\end{align*}
Let $\G$ be a sigma-algebra with $\G \subseteq \F$, then: 
\begin{align*}
\E_{Q}[X|\G] &= 
\frac{
\E[fX|\G]
}{
\E[f|\G]
}\;\; \text{a.s}
\end{align*}
\end{theorem}

\centerline{\textbf{Stochastic Differential Equations}}
\begin{definition}[\textbf{1-dimensional Ito process}]
Let $F\in M^{1}_{loc}([a,b])$  and $G\in M^{2}_{loc}([a,b])$, and $W = (W_{t})_{t\in [a,b]}$ be a one-dimensional standard Brownian motion on $(\Omega,\F, (\F_{t})_{t\in [a,b]},P)$  then a process on the form: 
\begin{align*}
X_{t} &= X_{a} + \int_{a}^{t}F_{s}ds + \int_{a}^{t}G_{s}dW_{s}    
\end{align*}
is called an Ito process, this can also be rewritten in differential form as: 
\begin{align*}
dX_{t} &= F_{t}dt + G_{t}dW_{t}    
\end{align*}
\end{definition} 

\begin{theorem}[\textbf{1-dimensional Ito formula \cite{oksendal2003stochastic}}]
\label{thm: Ito's_formula}
Let $X_{t}$ be an Ito process, given by:
$$
dX_{t} = F_{t}dt + G_{t}dW_{t}
$$
Let $g(t,x) \in C^{1,2}([0,\infty) \times \R)$ (one time differentiable in time, and twice differentiable in space), then $Y_{t} = g(t,X_{t})$ is again an Ito process and:
\begin{align*}
dY_{t} &= \frac{\partial g}{\partial t}(t,X_{t})dt + 
\frac{\partial g}{\partial x}(t,X_{t})dX_{t} + 
\frac{1}{2}\frac{\partial^{2}g}{\partial x^{2}}(t,X_{t})(dX_{t})^{2}
\end{align*}
where $(dX_{t})^{2} = dX_{t}\cdot dX_{t}$ is computed according to:
\begin{align*}
dt\cdot dt &= dt\cdot dW_{t} = dW_{t}\cdot dt = 0 \\ 
dW_{t}\cdot dW_{t} &= dt
\end{align*}
\end{theorem} 


\begin{theorem}[\textbf{Integral representation theorem w.r.t Brownian Motion \cite{baldi2017stochastic}}]
Let $W = (W_{t})_{t\geq 0}$ be an $m$-dimensional Brownian motion on $(\Omega, \F, (\overline{\F}_{t})_{t}, P)$, where $(\overline{\F}_{t})_{t}$ represents the augmented natural filtration. Let $T>0$, then we can represent every $Z\in L^{2}(\Omega, \overline{\F}_{T}, P)$ uniquely as: 
$$
Z = \E[Z] + \int_{0}^{T}H_{s}dW_{s}
$$
where $H \in M^{2}([0,T])$ is $(\overline{\F}_{t})_{t}$-adapted.
\end{theorem}

\begin{theorem}[\textbf{Martingale representation theorem \cite{baldi2017stochastic}}]
\label{thm: Martingale_rep_thm}
Let $M = (M_{t})_{t \in [0,T]}$ be a square integrable martingale with w.r.t $(\overline{\F}_{t})_{t}$. Then there exist a unique process $H\in M^{2}([0,T])$ such that: 
\begin{align*}
M_{t} &= \E[M_{T}] + \int_{0}^{T}H_{s}dW_{s} 
= M_{0} + \int_{0}^{T}H_{s}dW_{s} \;\; \text{a.s}
\end{align*}
\end{theorem}

Let $b(t,x) = (b_{i}(t,x))_{1\leq i \leq m}$ and
$\sigma(t,x) = (\sigma(t,x)_{ij})_{\substack{1\leq i\leq m\\1\leq j\leq d}}$ be measurable functions on $[0,T]\times \R^{m}$
\begin{definition}[\textbf{\cite{baldi2017stochastic}}]
Let $X = (X_{t})_{t\in [u,T]}$ be a stochastic process defined on $(\Omega, \F, (\F_{t})_{t\in [0,T]}, P)$, it is said to be a solution of the SDE \nomenclature{SDE}{Stochastic Differential Equation} (Stochastic differential equation)
\begin{align*}
(*)\begin{cases}
      dX_{t} &= b(t,X_{t})dt + \sigma(t,X_{t})dW_{t} \\
      X_{u} &= x \in \R^{m}
    \end{cases}       
\end{align*}
if: 
\begin{itemize}[leftmargin =*]
    \item $W = (W_{t})_{t\in [0,T]} \in \R^{d}$ is a Brownian motion on $(\Omega, \F, (\F_{t})_{t\in [0,T]}, P)$ and
    \item $\forall t \in [u,T]$ we have:
    $$
    X_{t} = x + \int_{u}^{t}b(s,X_{s})ds + \int_{u}^{T}\sigma(s,X_{s})dW_{s}
    $$
\end{itemize}
\end{definition}

\begin{definition}[\textbf{Strong solution}]
We say that equation \ref{eq: SDE} has strong solutions if for every standard Brownian motion $W = (W_{t})_{t}$ on $(\Omega, \F, (\F_{t})_{t}, P)$, there exists a process $X$ that satisfies equation \ref{eq: SDE}.    
\end{definition} 

\begin{definition}[\textbf{Uniqueness in distribution}]
We say that for the SDE in \ref{eq: SDE}, there is uniqueness in distribution if given two solutions $X^{i}$ on $(\Omega^{i}, \F^{i}, (\F_{t}^{i})_{t}, P^{i}), i = 1,2$ have the same distribution, i.e.
\[
X^{1} \stackrel{d}{=} X^{2}
\]
\end{definition}

\newpage 
\begin{theorem}[\textbf{\cite{baldi2017stochastic}}]
\label{thm: SDE_sufficiency}
Let $X = (X_{t})_{t\in [u,T]}$ be a stochastic process defined on $(\Omega, \F, (\F)_{t\in [0,T]}, P)$, furthermore let $\eta \in L^{2}(\Omega, \F, P)$ be $\F_{u}$-measurable and consider the SDE:
\begin{align}
\label{eq: SDE}
\begin{cases}
      dX_{t} &= b(t,X_{t})dt + \sigma(t,X_{t})dW_{t} \\
      X_{u}  &= \eta
    \end{cases}    
\end{align}
where $b, \sigma$ satisfies:
\begin{itemize}[leftmargin =*]
    \item $b,\sigma$ are measurable functions such that: $\exists L >0, M >0$ such that $\forall x,y \in \R^{m}, \forall t \in [u,T]$ 
    \begin{align*}
    |b(t,x)|                  &\leq M(1+|x|) \\ 
    |\sigma(t,x)|             &\leq M(1+|x|) \\ 
    |b(t,x) - b(t,y)|         &\leq L|x-y| \\ 
    |\sigma(t,x)+\sigma(t,y)| &\leq L|x-y|
    \end{align*}
\end{itemize}
Then $\exists X \in M^{2}([u,T])$ satisfying \ref{eq: SDE} and the solution is strong and strongly unique.
\end{theorem}












\newpage 
\begin{assumption}
Throughout this thesis, unless otherwise specified, we will assume that our probability space $(\Omega, \F, \mathbbm(F) = (\F_{t})_{t\geq 0}, P)$ are such that:
\begin{itemize}
    \item $\mathbbm{F}$ is $P$-augmented, i.e $\F_{t} = \overline{\F}_{t} = \sigma(\F_{t}\cup \mathcal{N})$ and
    \item $\mathbbm{F}$ is right-continuous, i.e. 
    \[
    \F_{t} = \F_{t^{+}}:= \bigcap_{u>t}\F_{u}
    \]
\end{itemize}
\end{assumption}
\newpage 






















\newpage

\newpage
\section{Levy processes}

\begin{definition}[\textbf{Levy process \cite{kenlévy}}]
A process $L = (L_{t})$ on $(\Omega, \F, P)$ is said to be a Levy-process if: 
\begin{itemize}[leftmargin=*]
    \item $L_{0} = 0$
    \item independent increments: $0 < s < t < s'< t'$: 
    $L_{t}-L_{s}$ is independent of $L_{t'}-L_{s'}$
    \item stationary increments: 
    $L_{t} -L_{s} \stackrel{\text{d}}{=} L_{t-s}$
    \item it's stochastically continuous, i.e 
    $$
    \forall \; \epsilon > 0: \;\; \lim\limits_{h \to 0}P(|X_{t+h}-X_{t}| \geq \epsilon) = 0
    $$
    \item the sample paths are cadlag
\end{itemize} 
\end{definition}


\begin{proposition}[\textbf{Characteristic function of Levy-process \cite{tankov2003financial}}]
Let $(L(t))_{t\geq 0}$ be a Levy-process on $\R^{d}$, then there exists a continuous function $\Psi: \R^{d} \to \R$ 
\nomenclature{$\Psi$}{Characteristic exponent of Levy process}
called the characteristic exponent of $L$ such that: 
$$
\E[e^{iuL(t)}] = e^{t\Psi(u)}
$$
\end{proposition}

\begin{definition}[\textbf{Random measure}]
Let $(E, \mathcal{E})$ be a measure space, a map $\mu : \Omega \times \mathcal{E} \to \R_{+}$ is a random measure if: 
\begin{itemize}[leftmargin=*]
    \item $\mu(\omega, \cdot)$ is a measure $\forall\; \omega \in \Omega$ 
    \item $\mu(\cdot, A)$ is a random variable $\forall\; A \in \mathcal{E}$
\end{itemize}
\end{definition} 

\begin{definition}[\textbf{Poisson random measure}]
Let $(E, \mathcal{E})$ be a measurable space, and $\nu: \mathcal{E}\to \R_{+}$ a $\sigma$-finite measure. 
\\~\\  
A random measure $\mu: \Omega \times \mathcal{E} \to \R_{+}$ is called a Poisson random measure with intensity measure $\nu$, if: 
\begin{itemize}[leftmargin=*]
    \item $\mu(\cdot, A) \sim Pois(\nu(A)) \; \forall A \in \mathcal{E}$ with $\nu(A) < \infty$
    \item $\mu(\cdot, A_{1}), \dots, \mu(\cdot, A_{n})$ are independent for $\{A_{n}\}_{n\in \N }$ disjoint. 
\end{itemize}
\end{definition} 

\begin{theorem}
Let $L = (L(t))_{t\geq 0}$ be a real valued Levy-process, define for $A \in \mathcal{B}(\R)\setminus \{0\}$, $t\geq 0, \omega \in \Omega$: 
\begin{align*}
N(t, A)(\omega) &= \#\{
s\leq t: \Delta L(s)(\omega) \in A
\} \\ 
&= 
\sum_{s\leq t}\mathbbm{1}_{A}(\Delta L(s)(\omega))
\end{align*}

a) if $A$ is bounded away from zero, then: 
$
N(t,A) < \infty \; a.s
$
and $t\mapsto N(t,A)$ is a Poisson process
\\~\\ 
b) $N(t, \cdot)$ is a Poisson random measure, and for all $t\geq 0$ with $\sigma$-finite $\nu$: 
\begin{align*}
\E[N(t,A)] &= \nu(A)t \;\; \text{and}\;\; \int_{\R\setminus\{0\}}\min(1, z^{2})\nu(dz) < \infty    
\end{align*}
\end{theorem} 

\begin{theorem}[\textbf{Levy-Khintchine theorem}]
\label{thm: Levy_Khintchine}
Let $L = (L_{t})_{t\geq 0}$ be a Levy-process, and $\nu$ a Levy measure, if $u \in \C$ such that: 
$$
\int_{|x|>1}e^{Re(u)}\nu(dx) < \infty
$$
then the Characteristic function is given by:
$$
\E[e^{iuL(t)}] = \exp\left(
t\Psi(u)
\right)
$$
with: 
\begin{align*}
\Psi(u) &= \gamma iu - \frac{u^{2}\sigma^{2}}{2}
+ \int_{\R\setminus\{0\}}[e^{iux}-1-iux\mathbbm{1}(|x|<1)]\nu(dx)
\end{align*}
The triplet $(\gamma, \sigma, \nu)$ defines the law of $L$ uniquely and is called the characteristic triplet. 
\end{theorem} 



\newpage 

\subsection{Compound Poisson Process (CPP)}

\begin{definition}[\textbf{Compound Poisson process}]
\label{def: CPP}
A compound Poisson process (CPP) \nomenclature{CPP}{Compound Poisson process} with intensity $\lambda > 0$ and jump size distribution $F_{J}(dx)$  is a stochastic process: 
$$
Y(t) = \sum_{i=1}^{N(t)}J_{k}
$$
where $J_{k}$ are iid with distribution $F_{J}(dx)$ and $N(t)$ is a Poisson process with intensity $\lambda$, independent of $(J_{k})_{k\geq 1}$
\end{definition}

\begin{proposition}[\textbf{Charactersitc function of CPP \cite{tankov2003financial}}]
\label{prop: characteristic_function_CPP}
The characteristic function of a CPP $I(t)$ is given by: 
\begin{align*}
\E[e^{iuI(t)}] &= \exp\left(
\lambda t \int_{\R}(e^{iux}-1)F_{J}(dx)
\right)    
\end{align*}
\end{proposition} 

\begin{proof}

\begin{align*}
\E[e^{iuI(t)}] &= 
\E\left[
e^{iu \sum_{k=1}^{N_{t}}J_{k}}
\right] \\
&= 
\E\left[\E[e^{iu \sum_{k=1}^{N_{t}}J_{k}}|N_{t}=n]\right] \\ 
&= 
\sum_{n\in \N_{0}}\E\left[e^{iu\sum_{k=1}^{n}J_{k}}\right]P(N_{t}=n) \\
&= 
\sum_{n\in \N}\prod_{k=1}^{n}\E[e^{iuJ_{k}}]\cdot e^{-\lambda t}\frac{(\lambda t)^{n}}{n!} \\ 
&= 
\sum_{n\in \N}\left(
\E\left[e^{iuJ_{1}}\right]
\right)^{n}e^{-\lambda t}\frac{(\lambda t)^{n}}{n!} \\ 
&= 
e^{-\lambda t}\sum_{n \in \N}\frac{
(\lambda t \E[e^{iuJ}])^{n}
}{
n!
} \\ 
&= e^{-\lambda t}e^{\lambda t \E[e^{iuJ}]} \\ 
&= \exp\left(
\lambda t(\E[e^{iuJ}] -1)
\right) \\ 
&= 
\exp\left(
\lambda t \left(\int_{\R}e^{iux}F_{J}(dx) - 1\right) 
\right) \\ 
&= 
\exp\left(
\lambda t \int_{\R}\left[e^{iux}-1 \right]F_{J}(dx)
\right)
\end{align*}

\end{proof}

\newpage 

\begin{proposition}[\textbf{Characteristic triplet of CPP}]
Let $I(t)$ be a CPP as described in Definition \ref{def: CPP}, we then have that the characteristic triplet is given by:
\begin{align*}
(\gamma, \sigma, \nu) &= 
(\lambda x \mathbbm{1}(|x|<1), 0, \lambda F_{J})
\end{align*}
\end{proposition}

\begin{proof}
From what we already have established, in combination with Levy Khintchine formula, we have that: 
\begin{align*}
\E[e^{iuI(1)}] &= 
\exp\left(
\lambda\int_{\R\setminus\{0\}}(e^{iux}-1)F_{J}(dx)
\right)    
 = \exp(\Psi(u))   
\end{align*}

Thus: 
\begin{align*}
\lambda\int_{\R\setminus\{0\}}(e^{iux}-1)F_{J}(dx)
&= 
\lambda iux\mathbbm{1}(|x|<1)F_{J}(dx) + 
\int_{\R\setminus\{0\}}[e^{iux}-1-iux\mathbbm{1}(|x|<1)]\nu(dx)
\end{align*}

Leaving us with $\nu(dx) = \lambda F_{J}(dx)$ and: 
\begin{align*}
(\gamma, \sigma, \nu) &= 
(\lambda x \mathbbm{1}(|x|<1), 0, \lambda F_{J})    
\end{align*}
\end{proof}

\begin{proposition}[\textbf{\cite{benth2008stochastic}}]
\label{prop: Integral_g(s)dI(s)}
Assume that $I$ is a CPP, $g$ a continuous function and that 
$s\mapsto \Psi(ug(s)) \in L^{1}([0,t], \F, P)$, then: 
\begin{align*}
\E\left[
\exp\left(
i\theta\int_{s}^{t}g(u)dI(u)
\right)
\right] 
&= 
\exp\left(
\int_{s}^{t}\Psi(\theta g(u))du
\right)
\end{align*}
Where $\Psi(x)$ is the cumulant function of $I(1)$ i.e: 
\begin{align*}
\Psi(x) &= 
\lambda \int_{\R}(e^{iyx}-1)F_{J}(dy)
\end{align*}
\end{proposition}

\begin{proof}
\nomenclature{DCT}{Dominated Convergence Theorem}
Since $g$ is a continuous function on $[s,t]$ we know that there exist $M>0$ such that $|g(u)| \leq M,\; \forall u \in [s,t]$, furthermore from Proposition \ref{prop: simple_functions_conv_pointwise_to_f}, we know that $g$ may be approximated by simple functions: 
\begin{align*}
h(u) &= \sum_{k=1}^{n}a_{k}\mathbbm{1}_{(u_{k-1}, u_{k}]}(u), \;\text{where:}\; s = u_{0} < u_{1} < \dots < u_{n} = t  
\end{align*}

\begin{align*}
\E\left[
\exp\left(
i\theta \int_{s}^{t}h(u)dI(u)
\right)
\right]
&= 
\E\left[
\exp\left(
i\theta \sum_{k=1}^{n}a_{k}[I(u_{k}) - I(u_{k-1})]
\right)
\right]
\end{align*}

Now as $I$ is a CPP (and therefore a Levy-process), we know that it has independent increments and has a stationary distribution, meaning that: 
$I(u_{k}) - I(u_{k-1}) \stackrel{d}{=} I(u_{k}-u_{k-1}) = I(\Delta_{k})$, leaving us with: 

\newpage 

\begin{align*}
\E\left[
\exp\left(
i\theta \sum_{k=1}^{n}a_{k}[I(u_{k}) - I(u_{k-1})]
\right)
\right] 
&= 
\prod_{k=1}^{n}\E\left[
\exp\left(
i\theta I(\Delta_{k})
\right)
\right] \\ 
&= 
\prod_{k=1}^{n}\exp\left(
\Psi(\theta a_{k})\Delta_{k}
\right) \\ 
&= 
\exp\left(
\sum_{k=1}^{n}\Psi(\theta a_{k})\Delta_{k}
\right)
\end{align*}

Thus: 
\begin{align*}
\E\left[
\exp\left(
i\theta\int_{s}^{t}g(u)dI(u)
\right)
\right]
&= 
\lim\limits_{\Delta_{k}\to 0}\E\left[
\exp\left(
i\theta\int_{s}^{t}h(u)dI(u)
\right)
\right] \\ 
&= 
\lim\limits_{\Delta_{k}\to 0}
\exp\left(
\sum_{k=1}^{n}\Psi(\theta a_{k})\Delta_{k}
\right) \\ 
&\stackrel{\mathclap{\mathrm{DCT}}}{=} 
\exp\left(
\lim\limits_{\Delta_{k}\to 0}
\sum_{k=1}^{n}\Psi(\theta a_{k})\Delta_{k}
\right) \\ 
&= 
\exp\left(
\int_{s}^{t}\Psi(\theta g(u))du
\right)
\end{align*}

\end{proof}

\newpage 


\subsection{Esscher Transform for CPP}
Let $Y\sim F_{Y}(dy)$, we then have: 
\[
\E[e^{\theta Y}] = \int_{\R}e^{\theta y}F_{Y}(dy)
\]

Furthermore, we denote: 
\begin{align*}
Z^{\theta}(T) &:= \frac{dQ^{\theta}}{dP} = \frac{e^{\theta Y}}{\E[e^{\theta Y}]}   
\end{align*} 

Now let $I(t)$ denote a CPP denote a CPP with intensity $\lambda$ and jump-distribution $J \sim F_{J}(dx)$, in this case we get: 

\begin{align*}
Z^{\theta}(T) &= \frac{
e^{\theta I(T)}
}{
\E[e^{\theta I(T)}]
}    
\end{align*} 

In order for $Z^{\theta}(T)$ to be well defined, we need $\E[e^{\theta I(T)}] < \infty $, now from Theorem \ref{thm: Levy_Khintchine} in combination with Proposition \ref{prop: characteristic_function_CPP}, we have:
\begin{align*}
\E[e^{\theta I(T)}] &= \exp(T\Psi(-i\theta))  
= 
\exp\left(
\lambda T(\E[e^{\theta J}] -1)
\right)
\end{align*} 

Meaning that we need $\E[e^{\theta J}] < \infty $ for $Z^{\theta}(T)$ to be well defined. 

\begin{notation}
For simplicity and ease of notation, we define: 
\begin{align*}
\xi(\theta) := \Psi(-i\theta) = \lambda(\E[e^{\theta J}] -1)  
\end{align*}
\nomenclature{$\xi$}{Transformed characteristic exponent of Levy process, $\xi(\theta):= \Psi(-i\theta)$}
\end{notation}

We then see that we can rewrite $Z^{\theta}(T)$ as: 
\begin{align*}
Z^{\theta}(T) &= e^{\theta I(T) - \xi(\theta)T}    
\end{align*}

\begin{proposition}
$Z^{\theta} = (Z^{\theta}(t))_{t\in [0,T]}$ is a $(P, \mathbbm{F})$-martingale.     
\end{proposition}

\begin{proof}
Let $0 \leq s \leq t \leq T$: 
\begin{align*}
\E[Z^{\theta}(t)|\F_{s}] &= 
\E\left[
e^{\theta I(t) - \xi(\theta)t}
\bigg{|}\F_{s}\right] \\ 
&= 
e^{-\xi(\theta)t}\E\left[
e^{\theta[I(s) + (I(t)-I(s))]}
\bigg{|}\F_{s}\right] \\ 
&= 
e^{-\xi(\theta)t}e^{\theta I(s)}\E\left[
e^{\theta I(t-s)}
\right] \\ 
&= 
e^{-\xi(\theta)t}e^{\theta I(s)}e^{\xi(\theta)(t-s)} \\ 
&= e^{\theta I(s) - \xi(\theta)s} \\ 
&= Z^{\theta}(s)
\end{align*}
\end{proof}

\newpage 

\begin{proposition}[\textbf{\cite{benth2008stochastic}}]
\label{prop: Esscher_transform_CPP_Q}
$I(t)$ is a CPP under $Q^{\theta}$ with intensity $\lambda_{Q^{\theta}} = \lambda \E[e^{\theta J}]$
\end{proposition} 

\begin{proof}
We start off by calculating the characteristic function under $Q^{\theta}$: 
\begin{align*}
\E_{Q^{\theta}}\left[
e^{iuI(t)} 
\right]
&= 
\E\left[
e^{iuI(t)}Z^{\theta}(t)
\right] \\ 
&= 
\E\left[
e^{iuI(t)}e^{\theta I(t) - \xi(\theta)t}
\right] \\ 
&= 
\exp(-\xi(\theta)t)\exp\left(
\lambda t(\E[e^{(iu+\theta)J}]-1)
\right) \\ 
&= 
\exp\left(
\lambda t(\E[e^{(iu+\theta)J}]) -
\lambda t(\E[e^{\theta J}]-1)
\right) \\ 
&= 
\exp\left(
\lambda t\E[e^{\theta J}(e^{iuJ}-1)]\cdot \frac{\E[e^{\theta J}]}{\E[e^{\theta J}]}
\right) \\ 
&= 
\exp\left(
\lambda t \E[e^{\theta J}]\E[Z^{\theta}(1)(e^{iuJ}-1)]
\right) \\ 
&= 
\exp\left(
t \underbrace{\lambda \E[e^{\theta J}]}_{= \lambda_{Q^{\theta}}}
(\E_{Q^{\theta}}[e^{iuJ}]-1)
\right)
\end{align*}

Thus: 
\begin{align*}
\E_{Q^{\theta}}\left[
e^{iuI(t)}
\right]
&= \exp\left(
t \Psi_{Q^{\theta}}(u)
\right), \;\text{where:}\;
\Psi_{Q^{\theta}}(u) = \lambda_{Q^{\theta}}\left(
\E_{Q^{\theta}}[e^{iuJ}]-1
\right)
\end{align*}
\end{proof}













