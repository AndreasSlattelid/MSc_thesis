\chapter{Theoretical Background}
\label{chp_theoretical_background}


\section{Measure Theory}
The measure theory results have been gathered from \cite{lindstrom2017}

\begin{definition}[\textbf{Sigma-algebra}]
Assume that $X$ is a non-empty set, a family $\F$ of subsets of $X$ is called a sigma-algebra if the following holds: 
\begin{enumerate}[label= (\roman*), leftmargin=*]
  \item $\emptyset \in \F$
  \item If $A\in \F$, then $A^{C}\in \F$ 
  \item If $A_{n} \in \F$ for all $n\in \N$, then $\bigcup_{n\in \N}A_{n}\in \F$
\end{enumerate}
\end{definition}

\begin{definition}[\textbf{Measure}]
Assume that $X$ is a non-empty set, and that $\F$ is a $\sigma$-algebra on $X$. 
A measure $\mu$ on $(X,\F)$ is a function $\mu:\F\to \overline{\R}_{+} =[0,\infty)\cup \{\infty\}$
\nomenclature{$\overline{\R}_{+}$}{Positive real line including infinity, i.e $[0,\infty]$}
such that: 
\begin{enumerate}[label = (\roman*), leftmargin=*]
    \item $\mu(\emptyset) = 0$ 
    \item if $\{A_{n}\}_{n\in \N}$ is a pairwise disjoint sequence, then: 
    \[\mu\left(\bigcup_{n\in \N}A_{n}\right) = \sum_{n\in \N}\mu(A_{n})\]
\end{enumerate}
We call the triplet $(X,\F,\mu)$ a measure space.
\end{definition}

\begin{proposition}[\textbf{Intersection of $\sigma$-algebras is a $\sigma$-algebra}]
Let $(X,\F, \mu)$ be a measure space, let $\mathcal{I}$ be a non-empty index set and let $\G_{i}$, $i\in \mathcal{I}$ be $\sigma$-algebras on $X$, then: 
\[\G = \bigcap_{i \in \mathcal{I}}\G_{i} = \{A\subseteq X: A\in \G_{i},\; \forall\; i\in \mathcal{I}\}
\] 
is a $\sigma$-algebra on $X$
\end{proposition}

\begin{proof}
Since all $\G_{i}$'s are $\sigma$-algebras, we have that $\emptyset \in \G_{i}\; \forall \; i \in \mathcal{I}$, thus $\emptyset \in \G$. 
\\~\\ 
Assume that $A \in \G$, meaning that $A \in \G_{i}\;\forall i \in \mathcal{I}$, now: since all $\G_{i}$'s are $\sigma$-algebras we have that $A^{C} \in \G$
\\~\\
Assume that $\{A_{n}\}_{n\in \N} \in \G$, then we have that $\{A_{n}\}_{n\in \N} \in \G_{i}\;\forall i\in \mathcal{I}$, and thus: $\bigcup_{n\in \N}A_{n} \in \G$.
\end{proof}


\begin{proposition}[\textbf{Continuity of measure}]
Let $\{A_{n}\}_{n\in \N}$ be a sequence of measurable sets in $(X, \F, \mu)$, then we have: 
\begin{enumerate}[label = (\roman*), leftmargin=*]
    \item Assume that $\{A_{n}\}_{n\in \N}$ is an increasing sequence, i.e that $A_{n} \subseteq A_{n+1}$ for all $n\in \N$, then: 
    \[\mu\left(\bigcup_{n\in \N}A_{n}\right) = \lim_{n\to \infty}\mu(A_{n})
    \]
    \item Assume that $\{A_{n}\}_{n\in \N}$ is a decreasing sequence, i.e that $A_{n+1} \subseteq A_{n}$ for all $n\in \N$, and that $\mu(A_{1}) < \infty$ then: 
    \[\mu\left(\bigcap_{n\in \N}A_{n}\right) = \lim_{n\to \infty}\mu(A_{n})
    \]
\end{enumerate}
\end{proposition}

\begin{definition}[\textbf{Null set}]
A set $N\subseteq X$ is called a null set, if there is a set $B\in \F$ such that $N\subseteq B$ and $\mu(B) = 0$. 
\end{definition}

\begin{definition}[\textbf{Complete measure space}]
A measure space $(X,\F, \mu)$ is called complete if all null sets belong to $\F$.
\end{definition}

Let $\mathcal{N}$ denote the collection of all null sets.

\begin{theorem}[\textbf{Complete measure space with complete measure}]
Assume that $(X, \F, \mu)$ is a measure space, and let:\\
$\overline{\F} = \{A\cup N: A\in \F \; and\; N\in \mathcal{N} \}$, define $\overline{\mu}:\overline{\F} \to \overline{\R}_{+}$ by: 
\[\overline{\mu}(A\cup N) = \mu(A), \;\; \forall A \in \F
\]
Then $(X,\overline{\F}, \overline{\mu})$ is a complete measure space extending $(X,\F,\mu)$. 
\end{theorem}

\begin{proposition}
Let $X$ be a nonempty set, and $\mathcal{A}$ a collection of subsets of $X$. Then there exists a smallest $\sigma$-algebra $\sigma(\mathcal{A})$ containing $\mathcal{A}$. Such that if $\mathcal{C}$ is any other $\sigma$-algebra containing $\mathcal{A}$ then $\sigma(\mathcal{A}) \subseteq \mathcal{C}$. 
\end{proposition}

\begin{definition}[\textbf{Borel $\sigma$-algebra}]
We define the Borel $\sigma$-algebra $\mathcal{B}$ as the smallest $\sigma$-algebra generated by all open sets on $\R$.
\end{definition}

\begin{example}[\textbf{Lesbegue Measure}]
Let $X = \R$ and $\mathcal{B}$ the Borel $\sigma$-algebra, the Lesbegue measure is a measure $\mu$ such that: 
\[
\mu([a,b]) = b-a
\]
\end{example}



\centerline{\textbf{Measurable functions}}
\begin{definition}[\textbf{Inverse image of $B$ under $f$}]
Let $X,Y$ be two non-empty sets, and let $f:X\to Y$ with $B\subseteq Y$, we then define the inverse image of $B$ as: 
\[f^{-1}(B) = \{x\in X: f(x)\in B\}
\]
\end{definition} 

We use the convention that $\overline{\R} = \R\cup \{-\infty, \infty\}$ \nomenclature{$\overline{\R}$}{Extended real-line i.e $[-\infty, \infty]$}


\begin{definition}[\textbf{Measurable function}]
Let $(X, \F, \mu)$ be a measure space. A function $f:X \to \overline{\R}$ is measurable if:
\begin{align*}
f^{-1}([-\infty, r)) \in \F   
\end{align*}
\end{definition}

\begin{proposition}
Assume that $f,g:X\to \R$ are measurable functions, then: 
\begin{enumerate}[label = (\roman*), leftmargin=*]
    \item $f+g$ is measurable.
    \item $f-g$ is measurable. 
    \item $fg$  is measurable.
\end{enumerate}
\end{proposition}


\centerline{\textbf{Integration of non negative functions}}

\begin{definition}[\textbf{Integration of simple function}]
\label{def: int_simple_function}
Assume that: 
\[
f(x) = \sum_{i=1}^{n}a_{i}\mathbbm{1}_{A_{i}}(x)
\]
is a non negative simple function on standard form i.e. $X = \bigcup_{i=1}^{n} A_{i}$ with $A_{i} = \{x\in X: f(x) = a_{i}\}$ disjoint and measurable. The integral of $f$ with respect to $\mu$ is defined as: 
\begin{align*}
\int fd\mu &= \sum_{i=1}^{n}a_{i}\mu(A_{i})    
\end{align*}
We use the convention that $0\cdot \infty = 0$
\end{definition}

\begin{definition}
If $f:X \to \overline{\R}_{+}$ is measurable we define: 
\begin{align*}
\int f d\mu 
&= 
\sup\left\{
\int gd\mu: \text{$g$ is a non negative simple function, $g\leq f$}
\right\}
\end{align*}
\end{definition}

\begin{proposition}
\label{prop: simple_functions_conv_pointwise_to_f}
If $f: X \to \overline{\R}_{+}$ is a measurable function, there exists an increasing sequence $\{h_{n}\}$ of simple functions converging pointwise to $f$. Moreover, for each $n$ and each $x\in X$ either:
\[
f(x) - \frac{1}{2^{n}} < h_{n}(x) \leq f(x) \;\;\text{or}\;\;
h_{n}(x) = 2^{n}
\]
\end{proposition}

\begin{theorem}[\textbf{Monotone Convergence Theorem}]
Assume that $f:X \to \overline{\R}_{+}$ is measurable, and assume that $\{f_{n}\}$ is an increasing sequence of non-negative measurable functions converging pointwise to $f$ so that $\lim\limits_{n\to \infty}f_{n}(x) = f,\; \forall x\in X$, then: 
\[ \lim_{n\to \infty}\int f_{n}d\mu = \int \lim_{n\to \infty}f_{n}d\mu
\]
\end{theorem}

\begin{theorem}[\textbf{Fatou's lemma}]
Let $\{f_{n}\}$ be a sequence of non-negative measurable functions, then: 
\[\liminf_{n\to \infty}\int f_{n}d\mu \geq \int \liminf_{n\to \infty}f_{n}d\mu
\]
\end{theorem}

\begin{definition}
A function $f:X \to \overline{\R}_{+}$ is said to be integrable if it is measurable and $\int f d\mu < \infty$    
\end{definition}


\newpage 
\centerline{\textbf{Integration of general functions}}
We would also like to integrate functions taking negative values as well, we then observe that if $f:X \to \overline{\R}$ then $f = f_{+} - f_{-}$ with: 
\begin{align*}
f_{+}(x) &= \begin{cases}
      f(x) & \text{$f(x) \geq 0$}\\
      0 & \text{otherwise}
    \end{cases} 
\;\;\;    
f_{-}(x) = \begin{cases}
      -f(x) & \text{$f(x) < 0$}\\
      0 & \text{otherwise}
    \end{cases}      
\end{align*}

\begin{definition}
A function $f: X \to \overline{\R}$ is called integrable it if is measurable, and $f_{+}$ and $f_{-}$ are integrable, we define the integral of $f$ as: 
\begin{align*}
\int fd\mu &= \int f_{+}d\mu - \int f_{-}d\mu    
\end{align*}
\end{definition} 

\begin{lemma}
A measurable function $f$ is integrable if and only if it's absolute value $|f|$ is integrable i.e. if and only if: 
\begin{align*}
\int |f| d\mu < \infty    
\end{align*}
\end{lemma}

\begin{theorem}[\textbf{Lebesgue's Dominated Convergence Theorem}]
Assume that $g:X \to \overline{\R}_{+}$ is a non-negative, integrable function and that $\{f_{n}\}$ is a sequence of measurable functions converging pointwise to $f$. 
If $|f_{n}| \leq g$ for all $n$, then: 
\begin{align*}
\lim\limits_{n \to \infty}\int f_{n}d\mu &= \int fd\mu    
\end{align*}
\end{theorem} 

\centerline{\textbf{Riemann and Lesbegue integration}}

\begin{theorem}
Assume that $f:[a,b]\to [0,\infty)$ is a bounded Riemann integrable function on $[a,b]$. Then $f$ is measurable and the Riemann and Lebesgue integral coincide:
\[
\int_{a}^{b}f(x)dx = \int_{[a,b]}fd\mu
\]
\end{theorem}

\centerline{\textbf{$L^{p}$-spaces}}

\begin{definition}{\textbf{$\mathcal{L}^{p}$}}
If $1 \leq p < \infty$ and $(X,\F, \mu)$ is a measure space, we define: 
\begin{align*}
\mathcal{L}^{p}(X,\F, \mu) = 
\{
f:X \to \overline{\mathbbm{C}}: f\:\text{is measurable and}
\int |f|^{p}d\mu < \infty
\}
\end{align*}
furthermore, define: 
\begin{align*}
\norm{f}_{p} = \left(\int |f|^{p}d\mu \right)^{\frac{1}{p}}    
\end{align*}
\end{definition}

\begin{definition}[$L^{p}$]
Let $p \in [1,\infty)$, and define a relation by: 
\[
f \sim g \iff f = g\;\; a.e.
\] 
Consider the equivalence class:
\[
[f]:= \{g \in \mathcal{L}^{p}: g \sim f\}
\]
We then define: 
\begin{align*}
L^{p}(X,\F, \mu) := \{[f]: f\in \mathcal{L}^{p}\}    
\end{align*}
\end{definition}











\newpage 

\section{Probability theory}

Most of the results in this section are gathered from \cite{walsh2012knowing}, let $\F$ be a $\sigma$-algebra and let $\Omega$ denote the sample space. 

\begin{definition}[\textbf{Probability measure}]
A probability measure $P$ on $(\Omega, \F)$ is a function $P:\F \to [0,1]$ such that: 
\begin{enumerate}[label = (\roman*), , leftmargin=*]
    \item if $A \in \F $, then $P(A)\geq 0$
    \item $P(\Omega) = 1$
    \item if $\{A_{n}\}_{n\in \N}$ is a pairwise disjoint sequence, then: 
    \[P\left(\bigcup_{n\in \N}A_{n}\right) = \sum_{n\in \N}P(A_{n})\]
\end{enumerate}
\end{definition}

\begin{definition}[\textbf{Random Variable}]
Let $(\Omega, \F, P)$ be a probability space. A random variable $X$ is a function $X:\Omega \to \R$ such that: 
\begin{align*}
\{\omega: X(\omega) \leq x\} \in \F   
\end{align*}
\end{definition}

\begin{proposition}
Let $X$ be a random variable, and let $A \in \mathcal{B}$ (the Borel $\sigma$-algebra), then: 
\begin{align*}
\{X \in A\} \in \F    
\end{align*}
\end{proposition}

\centerline{\textbf{Expectations and Conditional Expectations}}
We will now use the results from measure theory and see how it relates to the construction of expectations and conditional expectations. 

\begin{definition}[\textbf{Discrete random variable}]
We say that a random variable $X$ is discrete if: 
\[
X(\omega) = \sum_{i=1}^{\infty}x_{i}\mathbbm{1}_{A_{i}}(\omega)
\]
Where $A_{i} = \{X = x_{i}\}$, furthermore we assume that it is on standard-form (See Definition \ref{def: int_simple_function})
\end{definition}

\begin{definition}[\textbf{Expectation Discrete case}]
Let $X$ be a discrete random variable, we say that $X$ is integrable if:
\[
\sum_{i=1}^{\infty}|x_{i}|P(A_{i}) < \infty
\]
If $X$ is integrable, we define the expectation as:
\[
\E[X] = \sum_{i=1}^{\infty}x_{i}P(A_{i})
\]
\end{definition}

In order to define the expectation of a general random variable $X$ one also consider sequences of non-negative simple functions, and decomposes the expectation in two positive random variables, i.e. $X = X^{+} - X^{-}$ whit $X^{+} = \max(X,0)$ and $X^{-} = \max(-X,0)$, and define the expectation as: 
\[
\E[X] = \E[X^{+}] - \E[X^{-}]
\]

We will mostly consider the expectation as a measure-theoretic integral, i.e: 
\[
\E[X] = \int_{\Omega}X(\omega)dP(\omega)
\]


\begin{definition}[\textbf{Conditional Expectation}]
Let $(\Omega, \F, P)$ be a probability space, let $X$ be an integrable random variable, and let $\mathcal{G} \subseteq \F$ be a sub $\sigma$-algebra, we say that a random variable $Z = \E[X|\mathcal{G}]$ is the conditional expectation of $X$ given $\mathcal{G}$ if: 
\begin{enumerate}[label= (\roman*), , leftmargin=*]
    \item $Z$ is $\mathcal{G}$-measurable, and
    \item if $A\in \mathcal{G}$, then:
    \[
    \int_{A}ZdP = \int_{A}XdP
    \]
\end{enumerate}
\end{definition}

\begin{theorem}
\label{thm: Conditional_expectation_rules}
Let $X$ and $Y$ be integrable random variables, let $a,b \in \R$ and let $\mathcal{G} \subseteq \F$ be a sub $\sigma$-algebra, then: 
\begin{enumerate}[label= (\roman*), , leftmargin=*]
    \item $\G = \{\emptyset, \Omega\}$, then: $\E[X|\G] = \E[X]$
    \item $\E[\E[X|\G]] = \E[X]$
    \item If $X$ is $\G$-measurable, $\E[X|\G] = X$ a.e.
    \item $\E[aX+bY|\G] = a\E[X|\G] + b\E[Y|\G]$ a.e. 
    \item If $X\geq 0$ a.e., $\E[X|\G] \geq 0$ a.e. 
    \item If $X\leq Y$ a.e., $\E[X|\G] \leq \E[Y|\G]$ a.e. 
    \item $|\E[X|\G]| \leq \E[|X||\G]$ a.e. 
    \item Suppose that $Y$ is $\G$-measurable and $XY$ are integrable, then: 
    \[
    \E[XY|\G] = Y\E[X|\G]\;\;a.e.
    \]
    \item If $X$ and $\G$ are independent, then: 
    \[
    \E[X|\G] = \E[X]\;\; a.e.
    \]
    \item If $X_{n}$ and $X$ are integrable, and either $X_{n} \uparrow X$, or $X_{n} \downarrow X$, then: 
    \[
    \E[X_{n}|\G] \to \E[X|\G]\;\; a.e.
    \]
\end{enumerate}
\end{theorem}

\begin{theorem}[\textbf{Tower Law}]
\label{thm: Tower_law}
If $X$ is an integrable random variable, and if $\G_{1} \subseteq \G_{2}$ are $\sigma$-algebras, then: 
\[
\E[\E[X|\G_{1}]|\G_{2}] = \E[\E[X|\G_{2}]|\G_{1}] = \E[X|\G_{1}]
\]
\end{theorem}

\begin{theorem}[\textbf{Jensen's inequality}]
\label{thm: Jensen's_ineuality}
Let $\phi$ be a convex function on an open interval $(x_{1}, x_{2})$ and let $X$ be a random variable whose range is in $(x_{1}, x_{2})$. Suppose $X$ and $\phi(X)$ are integrable and that $\G \subseteq \F$ are $\sigma$-algebras, then: 
\[
\phi\left(\E[X|\G]\right) \leq \E\left[\phi(X)|\G \right]\;\;a.e.
\]
\end{theorem}

\newpage 
\section{Stochastic Analysis}
The results in this section are based on \cite{walsh2012knowing} and \cite{baldi2017stochastic}. 
\\~\\
\centerline{\textbf{Stochastic processes and filtrations}}

\begin{definition}[\textbf{Filtration}]
Let $\T$ denote an index set either countable or a subset of $\R$, we say that the collection $\mathbbm{F} = (\F_{t})_{t \in \T}$ of $\sigma$-algebras is a filtration if for every $s\leq t \in \T$: 
\[
\F_{s}\subseteq \F_{t}
\]
\end{definition}

\begin{definition}[\textbf{Augmented Filtration}]
The augmented filtration is the filtration obtained by including the collection of null sets $\mathcal{N}$ to the $\sigma$-algebra $\F_{t} = \sigma(X_{u}:u\leq t)$, i.e:
\[
\overline{\F}_{t} = \sigma(\F_{t}\cup \mathcal{N})
\]
\end{definition}

\begin{definition}[\textbf{Stochastic process}]
Let $(\Omega, \F, (\F_{t})_{t\in \T}, P)$ denote a probability equipped with a filtration $(\F_{t})_{t\in \T}$.\\ 
A stochastic process $X = (X_{t})_{t\in \T}$ is a collection of random variables defined on $(\Omega, \F)$ taking values in a measurable space $(E, \mathcal{E})$    
\end{definition}

\begin{definition}[\textbf{Adapted process}]
We say that the stochastic process $X = (X_{t})_{t\in \T}$ is adapted to the filtration $\mathbbm{F} = (\F_{t})_{t \in \T}$ if for every $t\in \T$ we have that $X_{t}$ is $\F_{t}$-measurable. 
\end{definition}

\begin{definition}[\textbf{Modification and Indistinguishable processes}]
Let $(\Omega, \F, (\F_{t})_{t\geq 0}, P) = (\Omega', \F',(\F_{t})_{t\geq 0}, P')$, we say that $X$ is a modification of $X'$ if:
\[
\forall\; t \;\; P(X_{t} = X_{t}') = 1
\]
We say that $X$ is indistinguishable from $X'$ if:
\[
P(X_{t} = X_{t}'\;\forall\; t) = 1
\]
\end{definition}

\begin{definition}[\textbf{$\sigma$-finite measure \cite{lindstrom2017}}]
We say that a measure space $(X, \F, \mu)$ is $\sigma$-finite if $X$ is a countable union of sets with finite measure, i.e for $\{A_{n}\}_{n\in \N} \in \F$ we have: 
\begin{align*}
X = \bigcup_{n\in \N}A_{n}\;\;\text{with}\;\;\mu(A_{n}) < \infty,\; \forall n \in \N    
\end{align*}
\end{definition}

\begin{theorem}[\textbf{\cite{lindstrom2017}}]
Assume that $(X, \F, \mu)$ and $(Y, \G, \nu)$ are two measure spaces, and let $\F\otimes \G$ denote the $\sigma$-algebra generated by the measurable rectangles $F\times G, F \in \F, G\in \G$. Then there exists a measure $\mu\times \nu$ on $(\F\otimes \G)$ such that: 
\[
\mu\times\nu(F\times G) = \mu(F)\nu(G)\;\;\text{for all $F\in \F, G \in \G$}
\]
If $\mu$ and $\nu$ are $\sigma$-finite, this measure is unique and is called the product measure of $\mu$ and $\nu$. 
\end{theorem}

\begin{definition}[\textbf{Measurable Process}]
A stochastic process $X = (X_{t})_{t\geq 0}$ taking values on a measurable space $(E, \mathcal{E})$ is said to be measurable if:
\[
A\times \Omega \ni (t,\omega) \mapsto X_{t}(\omega) \in E
\]
is measurable (with $A\subseteq E$), i.e:
\begin{align*}
\forall B \in \mathcal{B}(E): \;\; 
\{
(t,\omega) \in A \times \Omega: X_{t}(\omega) \in B
\} \in \mathcal{B}(A) \otimes \F
\end{align*}
\end{definition}

\begin{definition}[\textbf{Progressively measurable process}]
A stochastic process $X = (X_{t})_{t\geq 0}$ is said to be progressively measurable w.r.t $\mathbbm{\F}$ if:
\begin{align*}
\forall t:\;\; 
[0,t]\times \Omega \ni (s,\omega) \mapsto X_{s}(\omega)
\end{align*}
is measurable w.r.t $\mathcal{B}([0,t])\otimes \F_{t}$
\end{definition}


\begin{theorem}[\textbf{Kolmogorov's continuity theorem}]
Let $D\subseteq \R^{m}$, be an open set, and consider the process $X = (X_{\theta})_{\theta \in D}$ and assume there exists $\alpha >0, \beta>0, C>0$ such that:
\[E[|X_{\theta_{1}} - X_{\theta_{2}}|^{\beta}] \leq C|\theta_{1}-\theta_{2}|^{m + \alpha}
\]
then there exists a continuous modification $\widetilde{X}$ of $X$. Furthermore $\widetilde{X}$ is Hölder continuous with exponent $\gamma < \frac{\alpha}{\beta}$ on all compact subsets $K\subseteq D$, i.e: 
\[|\widetilde{X}_{\theta_{1}} -\widetilde{X}_{\theta_{2}}| \leq C|\theta_{1}-\theta_{2}|^{\gamma}
\]
\end{theorem}

\centerline{\textbf{Integral Spaces}}

\begin{definition}[$M^{p}_{loc}$]
Let $M_{loc}^{p}([a,b])$ denote the space of equivalence classes of real-valued progressively measurable processes $X = (X_{t})_{t\geq 0} \in \R^{d}$ such that: 
\begin{align*}
\int_{0}^{\infty}|X_{s}|^{p}ds < \infty \;\; \text{a.s}    
\end{align*}
\end{definition}

\begin{definition}[$M^{p}$]
Let $M^{p}[a,b]$ denote the subspace of $M^{p}_{loc}[a,b]$ such that: 
\begin{align*}
\E\left[
\int_{a}^{b}|X_{s}|^{p}ds
\right] < \infty   
\end{align*}
\end{definition}

\newpage 
\centerline{\textbf{Fubini and Stochastic Fubini}}
\label{thm: Fubini}
\begin{theorem}[\textbf{Fubini's theorem}]
Let $(X, \F, \mu)$ and $(Y, \G, \nu)$ be two $\sigma$-finite measure spaces, and assume that $f:X\times Y \to \overline{\R}$ is $\mu\times\nu$-integrable, i.e. 
\[
\iint |f(x,y)|d(\mu\times \nu) < \infty
\]
Then: 
\begin{align*}
x \mapsto \int f(x,y)d\nu(y) \;\;\text{and}\;\; y\mapsto \int f(x,y)d\mu(x)    
\end{align*}
are $\mu-$ and $\nu-$integrable, respectively. Moreover:
\begin{align*}
\int_{X\times Y} f d(\mu \times \nu) &=     
\int_{X} \left[
\int_{Y} f(x,y)d\nu(y)
\right]d\mu(x) 
= 
\int_{Y} \left[
\int_{X} f(x,y)d\mu(x)
\right]d\nu(y) 
\end{align*}

The functions $y \mapsto \int f(x,y)d\mu(x)$ and $x \mapsto \int f(x,y)d\nu(y)$ are defined $\mu$ (a.e.) and $\nu$ (a.e.) respectively. 
\end{theorem} 

\begin{theorem}[\textbf{Stochastic Fubini for Brownian Motion \cite{filipovic2009term}}]
\label{thm: Stochastic_Fubini}
Let $X = (X(\omega, t, s))_{[0\leq t,s\leq T]}$ be an $\R^{d}$-valued stochastic process satisfying: 
\begin{itemize}[leftmargin=*]
    \item $X$ is progressively measurable w.r.t $\F_{T}\otimes \mathcal{B}([0,T])$ 
    \item $\sup\limits_{0 \leq s,t\leq T}|{X(t,s)}| < \infty$
\end{itemize}
Then $\lambda(t) = \int_{0}^{T}X(t,s)ds \in M^{2}_{loc}[0,T]$ and there exists a $\F_{T}\otimes \mathcal{B}([0,T])$-measurable modification $\psi(s)$of $\int_{0}^{T}X(t,s)ds $ such that $\psi \in M^{2}_{loc}([0,T])$, moreover: 
\begin{align*}
\int_{0}^{T}\psi(s)ds &= \int_{0}^{T}\lambda(t)dW(t)    
\end{align*}
i.e. 
\begin{align*}
\int_{0}^{T}\left[
\int_{0}^{T}X(t,s)dW(t)
\right]ds 
&= 
\int_{0}^{T}\left[
\int_{0}^{T}X(t,s)ds
\right]dW(t) 
\end{align*}
\end{theorem}

\newpage 
\centerline{\textbf{Girsanov's theorem, Equivalent martingale measures and Bayes theorem}}

\begin{definition}[\textbf{Absolutely continuous measures}]
\label{def: absolutely_cont_measures}
Let $\mu$ and $\nu$ be two measures defined on $(X, \F)$, and define: 
\begin{align*}
\mathcal{N}_{\mu} &= \{A \in \F: \mu(A) = 0\} \\ 
\mathcal{N}_{\nu} &= \{A \in \F: \nu(A) = 0\} 
\end{align*} 
We say that $\nu$ is absolutely continuous w.r.t $\mu$ iff $\mathcal{N}_{\mu} \subseteq \mathcal{N}_{\nu}$ and we write $\nu \ll \mu$, i.e $\mu(A) = 0 \implies \nu(A) = 0$
\end{definition}

\begin{definition}[\textbf{Equivalent measures}]
Consider the situation as described in Definition \ref{def: absolutely_cont_measures}, we say that $\nu$ and $\mu$ are equivalent iff $\mathcal{N}_{\mu} \subseteq \mathcal{N}_{\nu}$ and 
$\mathcal{N}_{\nu} \subseteq \mathcal{N}_{\mu}$ i.e.  $\mathcal{N}_{\mu}= \mathcal{N}_{\nu}$
and we write $\nu \sim \mu$, i.e: 
$$
\mu(A) = 0 \iff \nu(A) = 0
$$
\end{definition}

\begin{theorem}[\textbf{Radon Nikodym derivative}]
Let $(X, \F, \mu)$ be a $\sigma$-finite measure space. Let $\nu$ be a $\sigma$-finite measure on $(X, \F)$ such that $\nu \sim \mu$. Then there exists a unique non-negative function $f$ on $X$ which is measurable w.r.t $\F$ for which: 
\begin{align*}
\nu(E) := \int_{E}fd\mu, \; \forall  E\in \F    
\end{align*}

$f$ is unique in the sense that if there is another non-negative measurable function $g$ such that:
\[
\nu(E) = \int_{E}gd\mu \implies f=g,\; \mu-\text{a.e}
\]
One usually denotes: 
\[
f = \frac{d\nu}{d\mu}
\]
\end{theorem} 

Let $W = (W_{t})_{t\in [0,T]} \in \R^{m}$ denote a Brownian motion on $(\Omega, \F, (\F_{t})_{t\in [0,T]}, P)$, furthermore let $\phi$ be an $\R^{m}$-valued process (also valid for $\C^{m}$) with \\
$\phi \in M^{2}_{loc}([0,T])$, the process we will be interested in looks like: 
\begin{align}
\label{eq: Dodeans_exponential}
Z_{t} := \mathcal{E}_{t}\left(
\phi \bullet W
\right) =
\exp\left(
\int_{0}^{t}\phi_{s}dW_{s} - \frac{1}{2}\int_{0}^{t}\phi_{s}^{2}ds
\right)
\end{align}

\begin{proposition}[\textbf{Application of Radon-Nikodym derivative}]
Let $Q\sim P$ and define 
$Q(A) := \E[Z_{T}\mathbbm{1}_{A}] = \int_{A}Z_{T}dP$, where $A \in \F$ and $Z$ is defined as in Equation \ref{eq: Dodeans_exponential}, furthermore require that $\E[Z_{T}] = 1$, then $Q$ defines a new probability measure on $(\Omega, \F)$, and $$
\frac{dQ}{dP}\bigg{|}_{\F_{T}} = Z_{T}
$$
\end{proposition}

\begin{proof}
Let $\{A_{n}\}_{n\in \N}$ be a sequence of disjoint sets in $\F$, we then have that $Q$ defines a measure as: 

\begin{align*}
Q(\emptyset) &= \int_{\emptyset}Z_{T}dP = 0 \\
1 &= \E[Z_{T}] = \int_{\Omega}Z_{T}dP = Q(\Omega)\\ 
Q\left(
\bigcup_{n\in \N}A_{n}
\right)
&= 
\int_{\bigcup_{n\in \N}A_{n}}Z_{T}dP
= \int_{\Omega}Z_{T}\mathbbm{1}_{\bigcup_{n\in \N}A_{n}}dP
= \sum_{n\in \N}Q(A_{n})
\end{align*}
As $Z = (Z_{t})_{t\geq 0}$ by construction is measurable as well as non-negative, it follows from Radon-Nikodym theorem that: 
\[
\frac{dQ}{dP}\bigg{|}_{\F_{T}} = Z_{T}
\]
\end{proof}

\begin{theorem}[\textbf{Girsanov's theorem \cite{baldi2017stochastic}}]
\label{thm: Girsanov's_thm}
Let $W = (W_{t})_{t\in [0,T]}$ be an $m$-dimensional Brownian motion on $(\Omega, \F, (\F_{t})_{t\in [0,T]}, P)$, let $Z = (Z_{t})_{t\in [0,T]}$  be defined as in equation \ref{eq: Dodeans_exponential} with $\phi \in M^{2}_{loc}[0,T]$. Furthermore assume that $Z$ is a martingale w.r.t $P$ and let $Q$ be a probability measure on $(\Omega, \F)$ defined via the Radon-Nikodym density $Z_{T}$, then: 
\begin{align*}
W_{t}^{Q} &= W_{t} - \int_{0}^{t}\phi_{s}ds    
\end{align*}
defines a $(Q,\F)$-Brownian motion on $[0,T]$
\end{theorem}

Often what makes Girsanov's theorem hard to use is the requirement of $Z$ being a martingale under $P$, therefore the next theorem is quite useful: 

\begin{theorem}[\textbf{\cite{baldi2017stochastic}}]
\label{thm: Novikov_cond_and_implications}
Let $\phi \in M^{2}_{loc}([0,T])$, define $M_{t} = \int_{0}^{t}\phi_{s}dW_{s}, t \in [0,T]$ with $\langle M \rangle_{t} = \int_{0}^{t}\phi^{2}_{s}ds$, and let: 
$$
Z_{t} = \exp\left(
M_{t} - \frac{1}{2}\langle M \rangle_{t}
\right)
$$
Consider the following properties: 
\begin{enumerate}[label= (\roman*), leftmargin=*]
    \item $\E\left[e^{\frac{1}{2}\int_{0}^{T}|\phi_{s}|^{2}ds}\right] < \infty$ (Novikov's condition)
    \item $M = (M_{t})_{t\in [0,T]}$ is a bounded martingale in $L^{2}(\Omega, \F, P)$ and \\ 
    $\E[e^{\frac{1}{2} M_{T}}] < \infty$ 
    \item $Z = (Z_{t})_{t\in [0,T]}$ is a uniformly integrable martingale. 
\end{enumerate}

Then $(i) \implies (ii) \implies (iii)$
\end{theorem} 

\begin{theorem}[\textbf{Bayes theorem \cite{oksendal2003stochastic}}]
\label{thm: Bayes_thm}
Let $P$ and $Q$ be two probability measures on $(\Omega, \F)$ such that $\frac{dQ}{dP} = f$ with $f\in L^{1}(\Omega, \F, P)$. Let $X$ be a random variable on $(\Omega, \F)$ such that: 
\begin{align*}
\E_{Q}[|X|] &= \int_{\Omega}|X(\omega)|f(\omega)dP(\omega) < \infty
\end{align*}
Let $\G$ be a sigma-algebra with $\G \subseteq \F$, then: 
\begin{align*}
\E_{Q}[X|\G] &= 
\frac{
\E[fX|\G]
}{
\E[f|\G]
}\;\; \text{a.s}
\end{align*}
\end{theorem}

\newpage 

\centerline{\textbf{Stochastic Differential Equations}}
\begin{definition}[\textbf{1-dimensional Ito process}]
Let $F\in M^{1}_{loc}([a,b])$  and $G\in M^{2}_{loc}([a,b])$, and $W = (W_{t})_{t\in [a,b]}$ be a one-dimensional standard Brownian motion on $(\Omega,\F, (\F_{t})_{t\in [a,b]},P)$  then a process on the form: 
\begin{align*}
X_{t} &= X_{a} + \int_{a}^{t}F_{s}ds + \int_{a}^{t}G_{s}dW_{s}    
\end{align*}
is called an Ito process, this can also be rewritten in differential form as: 
\begin{align*}
dX_{t} &= F_{t}dt + G_{t}dW_{t}    
\end{align*}
\end{definition} 

\begin{theorem}[\textbf{1-dimensional Ito formula \cite{oksendal2003stochastic}}]
\label{thm: Ito's_formula}
Let $X_{t}$ be an Ito process, given by:
$$
dX_{t} = F_{t}dt + G_{t}dW_{t}
$$
Let $g(t,x) \in C^{1,2}([0,\infty) \times \R)$ (one time differentiable in time, and twice differentiable in space), then $Y_{t} = g(t,X_{t})$ is again an Ito process and:
\begin{align*}
dY_{t} &= \frac{\partial g}{\partial t}(t,X_{t})dt + 
\frac{\partial g}{\partial x}(t,X_{t})dX_{t} + 
\frac{1}{2}\frac{\partial^{2}g}{\partial x^{2}}(t,X_{t})(dX_{t})^{2}
\end{align*}
where $(dX_{t})^{2} = dX_{t}\cdot dX_{t}$ is computed according to:
\begin{align*}
dt\cdot dt &= dt\cdot dW_{t} = dW_{t}\cdot dt = 0 \\ 
dW_{t}\cdot dW_{t} &= dt
\end{align*}
\end{theorem} 


\begin{theorem}[\textbf{Integral representation theorem w.r.t Brownian Motion \cite{baldi2017stochastic}}]
Let $W = (W_{t})_{t\geq 0}$ be an $m$-dimensional Brownian motion on $(\Omega, \F, (\overline{\F}_{t})_{t}, P)$, where $(\overline{\F}_{t})_{t}$ represents the augmented natural filtration. Let $T>0$, then we can represent every $Z\in L^{2}(\Omega, \overline{\F}_{T}, P)$ uniquely as: 
$$
Z = \E[Z] + \int_{0}^{T}H_{s}dW_{s}
$$
where $H \in M^{2}([0,T])$ is $(\overline{\F}_{t})_{t}$-adapted.
\end{theorem}

\begin{theorem}[\textbf{Martingale representation theorem \cite{baldi2017stochastic}}]
\label{thm: Martingale_rep_thm}
Let $M = (M_{t})_{t \in [0,T]}$ be a square integrable martingale with w.r.t $(\overline{\F}_{t})_{t}$. Then there exist a unique process $H\in M^{2}([0,T])$ such that: 
\begin{align*}
M_{t} &= \E[M_{T}] + \int_{0}^{T}H_{s}dW_{s} 
= M_{0} + \int_{0}^{T}H_{s}dW_{s} \;\; \text{a.s}
\end{align*}
\end{theorem}

Let $b(t,x) = (b_{i}(t,x))_{1\leq i \leq m}$ and
$\sigma(t,x) = (\sigma(t,x)_{ij})_{\substack{1\leq i\leq m\\1\leq j\leq d}}$ be measurable functions on $[0,T]\times \R^{m}$

\newpage 

\begin{definition}[\textbf{\cite{baldi2017stochastic}}]
Let $X = (X_{t})_{t\in [u,T]}$ be a stochastic process defined on $(\Omega, \F, (\F_{t})_{t\in [0,T]}, P)$, it is said to be a solution of the SDE \nomenclature{SDE}{Stochastic Differential Equation} (Stochastic differential equation)
\begin{align*}
(*)\begin{cases}
      dX_{t} &= b(t,X_{t})dt + \sigma(t,X_{t})dW_{t} \\
      X_{u} &= x \in \R^{m}
    \end{cases}       
\end{align*}
if: 
\begin{itemize}[leftmargin =*]
    \item $W = (W_{t})_{t\in [0,T]} \in \R^{d}$ is a Brownian motion on $(\Omega, \F, (\F_{t})_{t\in [0,T]}, P)$ and
    \item $\forall t \in [u,T]$ we have:
    $$
    X_{t} = x + \int_{u}^{t}b(s,X_{s})ds + \int_{u}^{T}\sigma(s,X_{s})dW_{s}
    $$
\end{itemize}
\end{definition}

\begin{definition}[\textbf{Strong solution}]
We say that equation \ref{eq: SDE} has strong solutions if for every standard Brownian motion $W = (W_{t})_{t}$ on $(\Omega, \F, (\F_{t})_{t}, P)$, there exists a process $X$ that satisfies equation \ref{eq: SDE}.    
\end{definition} 

\begin{definition}[\textbf{Uniqueness in distribution}]
We say that for the SDE in \ref{eq: SDE}, there is uniqueness in distribution if given two solutions $X^{i}$ on $(\Omega^{i}, \F^{i}, (\F_{t}^{i})_{t}, P^{i}), i = 1,2$ have the same distribution, i.e.
\[
X^{1} \stackrel{d}{=} X^{2}
\]
\end{definition}

\begin{theorem}[\textbf{\cite{baldi2017stochastic}}]
\label{thm: SDE_sufficiency}
Let $X = (X_{t})_{t\in [u,T]}$ be a stochastic process defined on $(\Omega, \F, (\F)_{t\in [0,T]}, P)$, furthermore let $\eta \in L^{2}(\Omega, \F, P)$ be $\F_{u}$-measurable and consider the SDE:
\begin{align}
\label{eq: SDE}
\begin{cases}
      dX_{t} &= b(t,X_{t})dt + \sigma(t,X_{t})dW_{t} \\
      X_{u}  &= \eta
    \end{cases}    
\end{align}
where $b, \sigma$ satisfies:
\begin{itemize}[leftmargin =*]
    \item $b,\sigma$ are measurable functions such that: $\exists L >0, M >0$ such that $\forall x,y \in \R^{m}, \forall t \in [u,T]$ 
    \begin{align*}
    |b(t,x)|                  &\leq M(1+|x|) \\ 
    |\sigma(t,x)|             &\leq M(1+|x|) \\ 
    |b(t,x) - b(t,y)|         &\leq L|x-y| \\ 
    |\sigma(t,x)+\sigma(t,y)| &\leq L|x-y|
    \end{align*}
\end{itemize}
Then $\exists X \in M^{2}([u,T])$ satisfying \ref{eq: SDE} and the solution is strong and strongly unique.
\end{theorem}












\begin{assumption}
Throughout this thesis, unless otherwise specified, we will assume that our probability space $(\Omega, \F, \mathbbm{F} = (\F_{t})_{t\geq 0}, P)$ are such that:
\begin{itemize}[leftmargin =*]
    \item $\mathbbm{F}$ is augmented with it's respective measure, i.e $\F_{t} = \overline{\F}_{t} = \sigma(\F_{t}\cup \mathcal{N})$ and
    \item $\mathbbm{F}$ is right-continuous, i.e. 
    \[
    \F_{t} = \F_{t^{+}}:= \bigcap_{u>t}\F_{u}
    \]
\end{itemize}
\end{assumption}






















\newpage

\newpage
\section{Levy processes}
Assume that $(\Omega, \F, P)$ is a complete probability space. 

\begin{definition}[\textbf{Levy process \cite{kenlévy}}]
A stochastic process $X = (X_{t})_{t\geq 0}$ is a Levy process if:
\begin{enumerate}[leftmargin =*]
    \item $X_{0} = 0$
    \item $X$ has independent increments, i.e $\forall \; 0\leq t < u:$
    \begin{align*}
    X_{u}-X_{t}\;\text{is independent of}\;X_{s}-X_{r}\; \forall\; 0 \leq r < s \leq t    
    \end{align*}
    \item $X$ has stationary increments, i.e. 
    \begin{align*}
    \forall\; 0 \leq s < t: \; X_{t}-X_{s} \stackrel{d}{=} X_{t-s}
    \end{align*}
    \item $X$ is stochastically continuous: 
    \begin{align*}
    \forall \; \epsilon > 0: \;\; \lim\limits_{h \to 0}P(|X_{t+h}-X_{t}| \geq \epsilon) = 0 
    \end{align*}
    \item $X$ has càdlàg \nomenclature{càdlàg}{right continuous with existing left limits} sample paths
\end{enumerate}
\end{definition}

\begin{definition}[\textbf{Infinitely divisible \cite{applebaum_lévy}}]
A random variable $X$ is said to be infinitely divisible if $\forall\; n\in \N$ there exist $X_{n,1}, \dots, X_{n,n}$ such that: 
\begin{align*}
X \stackrel{d}{=} \sum_{k=1}^{n}X_{n,k}
\end{align*}
\end{definition}

\begin{proposition}
If $X = (X_{t})_{t\geq 0}$ is a Levy-process, then $\forall\; t\geq 0$ $X_{t}$ is infinitely divisible. 
\end{proposition}

\begin{definition}[\textbf{Levy-measure}]
A Levy-measure is a Borel-measure \nomenclature{Borel-measure}{Measure defined on the $\sigma$-algebra of Borel sets} $\nu$  defined on $\R_{0}^{d} = \R^{d}\setminus \{0\}$ 
\nomenclature{$\R_{0}^{d}$}{$\R^{d}\setminus \{0\}$}
such that: 
\begin{align*}
\int_{\R_{0}^{d}}1\wedge|x|^{2}\nu(dx) < \infty    
\end{align*}
\end{definition}

\begin{theorem}[\textbf{Levy-Khintchine theorem \cite{applebaum_lévy}}]
\label{thm: Levy_Khintchine}
Let $\mu$ be a probability measure on $\R^{d}$, then there exist: 
\begin{itemize}[leftmargin =*]
    \item $\gamma \in \R^{d}$
    \item $A \in \R^{d\times d}$: Positive semi-definite symmetric matrix $(u^{Tr}Au \geq 0, \forall\; u\in \R^{d})$ 
    \item $\nu$ a Levy-measure on $\R_{0}^{d}$ such that $\forall \; u \in \R^{d}$: 
    \begin{align*}
    \varphi_{\mu}(u) &= \exp\left(
    i\langle \gamma, u \rangle - \frac{1}{2}\langle u, Au \rangle 
    + \int_{\R_{0}^{d}}\left[
    e^{i \langle u, x\rangle} - 1 - i\langle u, x\rangle\mathbbm{1}(|x|<1)
    \right]\nu(dx)
    \right)
    \end{align*}
\end{itemize}
$(\gamma, A, \nu)$ is called the characteristic triplet of $X$. 
\end{theorem} 

\newpage 

\begin{definition}[\textbf{Characteristic exponent}]
The function $\Psi: \R^{d} \to \C$: 
\begin{align*}
\Psi(u) &= i\langle \gamma, u \rangle - \frac{1}{2}\langle u, Au \rangle 
    + \int_{\R_{0}^{d}}\left[
    e^{i \langle u, x\rangle} - 1 - i\langle u, x\rangle\mathbbm{1}(|x|<1)
    \right]\nu(dx)    
\end{align*}
is called the characteristic exponent of the Levy-process $X$
\end{definition}


\begin{theorem}[\textbf{\cite{applebaum_lévy}}]
If $X = (X_{t})_{t\geq 0}$ is a Levy process, then the characteristic triplet of each random variable $X_{t}$ takes the following form: 
\begin{align*}
(\gamma_{(t)}, A_{(t)}, \nu_{(t)})
&= 
(t\gamma, tA, t\nu)
\end{align*}
where $\gamma, A$ and $\nu$ are as described in Theorem \ref{thm: Levy_Khintchine}, the characteristic function takes the following form: 
\begin{align*}
\E[e^{iuX(t)}] &= \exp(\Psi_{(t)}(u)) \\ 
 &= \exp(t\Psi_{(1)}(u))
\end{align*}
One uses the following convetion: 
\begin{align*}
\Psi(u):= \Psi_{(1)}(u) 
&= 
i\langle \gamma, u \rangle - 
\frac{1}{2}\langle u, Au \rangle 
+ \int_{\R_{0}^{d}}\left[
e^{i \langle u, x\rangle} - 1 - i\langle u, x\rangle\mathbbm{1}(|x|<1)
\right]\nu(dx)  
\end{align*}
\end{theorem}


\newpage 

\subsection{Compound Poisson Process (CPP)}

\begin{definition}[\textbf{Compound Poisson process}]
\label{def: CPP}
A compound Poisson process (CPP) \nomenclature{CPP}{Compound Poisson process} with intensity $\lambda > 0$ and jump size distribution $F_{J}(dx)$  is a stochastic process: 
$$
Y(t) = \sum_{i=1}^{N(t)}J_{k}
$$
where $J_{k}$ are iid with distribution $F_{J}(dx)$ and $N(t)$ is a Poisson process with intensity $\lambda$, independent of $(J_{k})_{k\geq 1}$
\end{definition}

\begin{proposition}[\textbf{Charactersitc function of CPP \cite{tankov2003financial}}]
\label{prop: characteristic_function_CPP}
The characteristic function of a CPP $I(t)$ is given by: 
\begin{align*}
\E[e^{iuI(t)}] &= \exp\left(
\lambda t \int_{\R}(e^{iux}-1)F_{J}(dx)
\right)    
\end{align*}
\end{proposition} 

\begin{proof}

\begin{align*}
\E[e^{iuI(t)}] &= 
\E\left[
e^{iu \sum_{k=1}^{N_{t}}J_{k}}
\right] \\
&= 
\E\left[\E[e^{iu \sum_{k=1}^{N_{t}}J_{k}}|N_{t}=n]\right] \\ 
&= 
\sum_{n\in \N_{0}}\E\left[e^{iu\sum_{k=1}^{n}J_{k}}\right]P(N_{t}=n) \\
&= 
\sum_{n\in \N}\prod_{k=1}^{n}\E[e^{iuJ_{k}}]\cdot e^{-\lambda t}\frac{(\lambda t)^{n}}{n!} \\ 
&= 
\sum_{n\in \N}\left(
\E\left[e^{iuJ_{1}}\right]
\right)^{n}e^{-\lambda t}\frac{(\lambda t)^{n}}{n!} \\ 
&= 
e^{-\lambda t}\sum_{n \in \N}\frac{
(\lambda t \E[e^{iuJ}])^{n}
}{
n!
} \\ 
&= e^{-\lambda t}e^{\lambda t \E[e^{iuJ}]} \\ 
&= \exp\left(
\lambda t(\E[e^{iuJ}] -1)
\right) \\ 
&= 
\exp\left(
\lambda t \left(\int_{\R}e^{iux}F_{J}(dx) - 1\right) 
\right) \\ 
&= 
\exp\left(
\lambda t \int_{\R}\left[e^{iux}-1 \right]F_{J}(dx)
\right)
\end{align*}

\end{proof}

\newpage 

\begin{proposition}[\textbf{Characteristic triplet of CPP}]
Let $I(t)$ be a CPP as described in Definition \ref{def: CPP}, we then have that the characteristic triplet of $I$ is given by:
\begin{align*}
(\gamma, A, \nu) &= 
\left(
\lambda \int_{|x|<1}xF_{J}(dx), 0, \lambda F_{J}
\right)
\end{align*}
\end{proposition}

\begin{proof}

From Proposition \ref{prop: characteristic_function_CPP}, we have: 
\begin{align*}
\E[e^{iuI(t)}] &= \exp\left(
\lambda t \int_{\R}(e^{iux}-1)F_{J}(dx)
\right) \\ 
&= 
\exp\left(t\Psi(u)\right)
\end{align*}

Now: 
\begin{align*}
\int_{\R}\left[
e^{iux}-1-iux\mathbbm{1}(|x|<1)
\right]\lambda F_{J}(dx) 
&= 
\lambda \int_{\R}[e^{iux}-1]F_{J}(dx) - iu\lambda\int_{\R}x\mathbbm{1}(|x|<1)F_{J}(dx) \\ 
&= 
\lambda \int_{\R}[e^{iux}-1]F_{J}(dx) - i \bigg{\langle} \lambda\int_{\R}x\mathbbm{1}(|x|<1)F_{J}(dx), u \bigg{\rangle}
\end{align*}

From this, we infer that: 
\begin{align*}
i\langle \gamma, u \rangle &- i\bigg{\langle} \lambda \int_{\R}x\mathbbm{1}(|x|<1)F_{J}(dx), u \bigg{\rangle}
= 0 \\ 
&\Updownarrow \\ 
\gamma &= \lambda \int_{|x|<1}xF_{J}(dx)
\end{align*}

\end{proof}

\begin{proposition}[\textbf{\cite{benth2008stochastic}}]
\label{prop: Integral_g(s)dI(s)}
Assume that $I$ is a CPP, $g$ a continuous function and that 
$s\mapsto \Psi(ug(s)) \in L^{1}([0,t], \F, P)$, then: 
\begin{align*}
\E\left[
\exp\left(
i\theta\int_{s}^{t}g(u)dI(u)
\right)
\right] 
&= 
\exp\left(
\int_{s}^{t}\Psi(\theta g(u))du
\right)
\end{align*}
Where $\Psi(x)$ is the cumulant function of $I(1)$ i.e: 
\begin{align*}
\Psi(x) &= 
\lambda \int_{\R}(e^{iyx}-1)F_{J}(dy)
\end{align*}
\end{proposition}

\begin{proof}
\nomenclature{DCT}{Dominated Convergence Theorem}
Since $g$ is a continuous function on $[s,t]$ we know that there exist $M>0$ such that $|g(u)| \leq M,\; \forall u \in [s,t]$, furthermore from Proposition \ref{prop: simple_functions_conv_pointwise_to_f}, we know that $g$ may be approximated by simple functions: 
\begin{align*}
h(u) &= \sum_{k=1}^{n}a_{k}\mathbbm{1}_{(u_{k-1}, u_{k}]}(u), \;\text{where:}\; s = u_{0} < u_{1} < \dots < u_{n} = t  
\end{align*}

\begin{align*}
\E\left[
\exp\left(
i\theta \int_{s}^{t}h(u)dI(u)
\right)
\right]
&= 
\E\left[
\exp\left(
i\theta \sum_{k=1}^{n}a_{k}[I(u_{k}) - I(u_{k-1})]
\right)
\right]
\end{align*}

Now as $I$ is a CPP (and therefore a Levy-process), we know that it has independent increments and has a stationary distribution, meaning that: 
$I(u_{k}) - I(u_{k-1}) \stackrel{d}{=} I(u_{k}-u_{k-1}) = I(\Delta_{k})$, leaving us with: 

\newpage 

\begin{align*}
\E\left[
\exp\left(
i\theta \sum_{k=1}^{n}a_{k}[I(u_{k}) - I(u_{k-1})]
\right)
\right] 
&= 
\prod_{k=1}^{n}\E\left[
\exp\left(
i\theta I(\Delta_{k})
\right)
\right] \\ 
&= 
\prod_{k=1}^{n}\exp\left(
\Psi(\theta a_{k})\Delta_{k}
\right) \\ 
&= 
\exp\left(
\sum_{k=1}^{n}\Psi(\theta a_{k})\Delta_{k}
\right)
\end{align*}

Thus: 
\begin{align*}
\E\left[
\exp\left(
i\theta\int_{s}^{t}g(u)dI(u)
\right)
\right]
&= 
\lim\limits_{\Delta_{k}\to 0}\E\left[
\exp\left(
i\theta\int_{s}^{t}h(u)dI(u)
\right)
\right] \\ 
&= 
\lim\limits_{\Delta_{k}\to 0}
\exp\left(
\sum_{k=1}^{n}\Psi(\theta a_{k})\Delta_{k}
\right) \\ 
&\stackrel{\mathclap{\mathrm{DCT}}}{=} 
\exp\left(
\lim\limits_{\Delta_{k}\to 0}
\sum_{k=1}^{n}\Psi(\theta a_{k})\Delta_{k}
\right) \\ 
&= 
\exp\left(
\int_{s}^{t}\Psi(\theta g(u))du
\right)
\end{align*}

\end{proof}

\newpage 


\subsection{Esscher Transform for CPP}
Let $Y\sim F_{Y}(dy)$, we then have: 
\[
\E[e^{\theta Y}] = \int_{\R}e^{\theta y}F_{Y}(dy)
\]

Furthermore, we denote: 
\begin{align*}
Z^{\theta}(T) &:= \frac{dQ^{\theta}}{dP} = \frac{e^{\theta Y}}{\E[e^{\theta Y}]}   
\end{align*} 

Now let $I(t)$ denote a CPP with intensity $\lambda$ and jump-distribution $J \sim F_{J}(dx)$, in this case we get: 

\begin{align*}
Z^{\theta}(T) &= \frac{
e^{\theta I(T)}
}{
\E[e^{\theta I(T)}]
}    
\end{align*} 

In order for $Z^{\theta}(T)$ to be well defined, we need $\E[e^{\theta I(T)}] < \infty $, now from Theorem \ref{thm: Levy_Khintchine} in combination with Proposition \ref{prop: characteristic_function_CPP}, we have:
\begin{align*}
\E[e^{\theta I(T)}] &= \exp(T\Psi(-i\theta))  
= 
\exp\left(
\lambda T(\E[e^{\theta J}] -1)
\right)
\end{align*} 

Meaning that we need $\E[e^{\theta J}] < \infty $ for $Z^{\theta}(T)$ to be well defined. 

\begin{notation}
For simplicity and ease of notation, we define: 
\begin{align*}
\xi(\theta) := \Psi(-i\theta) = \lambda(\E[e^{\theta J}] -1)  
\end{align*}
\nomenclature{$\xi$}{Transformed characteristic exponent of Levy process, $\xi(\theta):= \Psi(-i\theta)$}
\end{notation}

We can now rewrite $Z^{\theta}(T)$ as: 
\begin{align*}
Z^{\theta}(T) &= e^{\theta I(T) - \xi(\theta)T}    
\end{align*}

\begin{proposition}
$Z^{\theta} = (Z^{\theta}(t))_{t\in [0,T]}$ is a $(P, \mathbbm{F})$-martingale.     
\end{proposition}

\begin{proof}
Let $0 \leq s \leq t \leq T$: 
\begin{align*}
\E[Z^{\theta}(t)|\F_{s}] &= 
\E\left[
e^{\theta I(t) - \xi(\theta)t}
\bigg{|}\F_{s}\right] \\ 
&= 
e^{-\xi(\theta)t}\E\left[
e^{\theta[I(s) + (I(t)-I(s))]}
\bigg{|}\F_{s}\right] \\ 
&= 
e^{-\xi(\theta)t}e^{\theta I(s)}\E\left[
e^{\theta I(t-s)}
\right] \\ 
&= 
e^{-\xi(\theta)t}e^{\theta I(s)}e^{\xi(\theta)(t-s)} \\ 
&= e^{\theta I(s) - \xi(\theta)s} \\ 
&= Z^{\theta}(s)
\end{align*}
\end{proof}

\newpage 

\begin{proposition}[\textbf{\cite{benth2008stochastic}}]
\label{prop: Esscher_transform_CPP_Q}
$I(t)$ is a CPP under $Q^{\theta}$ with intensity $\lambda_{Q^{\theta}} = \lambda \E[e^{\theta J}]$
\end{proposition} 

\begin{proof}
We start off by calculating the characteristic function under $Q^{\theta}$: 
\begin{align*}
\E_{Q^{\theta}}\left[
e^{iuI(t)} 
\right]
&= 
\E\left[
e^{iuI(t)}Z^{\theta}(t)
\right] \\ 
&= 
\E\left[
e^{iuI(t)}e^{\theta I(t) - \xi(\theta)t}
\right] \\ 
&= 
\exp(-\xi(\theta)t)\exp\left(
\lambda t(\E[e^{(iu+\theta)J}]-1)
\right) \\ 
&= 
\exp\left(
\lambda t(\E[e^{(iu+\theta)J}]) -
\lambda t(\E[e^{\theta J}]-1)
\right) \\ 
&= 
\exp\left(
\lambda t\E[e^{\theta J}(e^{iuJ}-1)]\cdot \frac{\E[e^{\theta J}]}{\E[e^{\theta J}]}
\right) \\ 
&= 
\exp\left(
\lambda t \E[e^{\theta J}]\E[Z^{\theta}(1)(e^{iuJ}-1)]
\right) \\ 
&= 
\exp\left(
t \underbrace{\lambda \E[e^{\theta J}]}_{= \lambda_{Q^{\theta}}}
(\E_{Q^{\theta}}[e^{iuJ}]-1)
\right)
\end{align*}

Thus: 
\begin{align*}
\E_{Q^{\theta}}\left[
e^{iuI(t)}
\right]
&= \exp\left(
t \Psi_{Q^{\theta}}(u)
\right), \;\text{where:}\;
\Psi_{Q^{\theta}}(u) = \lambda_{Q^{\theta}}\left(
\E_{Q^{\theta}}[e^{iuJ}]-1
\right)
\end{align*}
\end{proof}

\begin{lemma}[\textbf{[Exercise MAT4770, Spring 2021]}]
\label{lemma: CPP_exp_mu}
Let $I(t)$ be a CPP under $P$ with intensity $\lambda$, and jump distribution $J\sim Exp(\mu)$, then for $\theta < \mu$, we have: 
\begin{align*}
\lambda_{Q} &= \frac{\lambda \mu}{\mu - \theta}
\;\text{and}\; J_{1} \sim Exp(\mu - \theta)
\end{align*}
\end{lemma}

\begin{proof}
For the Esscher transform to be well-defined, we must have that $\E[e^{\theta J}] < \infty$, now:
\begin{align*}
\E[e^{\theta J}] &= \int_{0}^{\infty}e^{\theta x}\mu e^{-\mu x}dx \\ 
&= \frac{\mu}{\theta - \mu}e^{(\theta - \mu)x}\bigg{|}_{0}^{\infty} \\ 
 &=
    \begin{cases}
      \infty & \theta \geq \mu\\
      \frac{\mu}{\theta - \mu} & \theta < \mu
    \end{cases} 
\end{align*}

To find the distribution of $J$ under $Q$, we can derive it's characteristic function: 
\begin{align*}
\E_{Q}[e^{iuJ}] &= \E_{Q}[e^{iuI(1)}] =  \exp\left(
\xi(iu+\theta) - \xi(\theta)
\right)   
\end{align*}

\begin{align*}
\xi(iu+\theta) - \xi(\theta) &= 
\lambda\left(\E[e^{(iu+\theta)J}]-1\right)
- 
\lambda\left(\E[e^{\theta J}]-1\right) \\ 
&= 
\lambda\E[e^{\theta J}(e^{iuJ}-1)] \\ 
&= 
\lambda \int_{0}^{\infty}e^{\theta x}[e^{iux}-1]F_{J}(dx)
\\ 
&= \lambda \int_{0}^{\infty}e^{\theta x}[e^{iux}-1]\mu e^{-\mu x}dx \cdot \frac{\theta - \mu}{\theta - \mu} \\ 
&= 
\frac{\lambda \mu}{\theta - \mu}\int_{0}^{\infty}[e^{iux}-1](\mu-\theta)e^{-(\mu - \theta)x}dx \\ 
&= 
\lambda_{Q}\int_{0}^{\infty}[e^{iux}-1]F_{J}^{Q}(dx)
\end{align*}

Meaning that $J \stackrel{Q}{\sim} Exp(\mu - \theta)$
\end{proof}












