\chapter{Estimating parameters for interest rate models} 

\begin{definition}[\textbf{ARMA(p,q)}]
A stochastic process $Y = (Y_{i})_{i\geq 1}$ is called an $ARMA(p,q)$ process, if it has the following representation: 
\begin{align*}
Y_{i} &= \mu + \sum_{j=1}^{p}\phi_{j}(Y_{i-j}-\mu) + \epsilon_{i} - \sum_{k=1}^{q}\theta_{k}\epsilon_{i-k} \end{align*}
Where: 
\begin{itemize}[leftmargin =*]
    \item $\epsilon_{i}$ are iid with $\E[\epsilon_{i}] = 0$ and $Var[\epsilon_{i}] = \sigma_{\epsilon}^{2}$ as well as independent of \\ 
    $\mathcal{Y}_{i-1} = (Y_{1}, \dots, Y_{i-1})$
\end{itemize}
\end{definition}


A special case of an $ARMA(p,q)$ is $AR(p)$, we have the the following relationship: 
\[
AR(p) = ARMA(p,0)
\]

We will be interested in $AR(1)$, meaning that: 
\[
Y_{i} = \mu + \phi[Y_{i-1}-\mu] + \epsilon_{i}
\]


Now recall that the Vasicek model looks like:
\begin{align*}
dr(t) &= \alpha[m - r(t)]dt + \sigma dW^{Q}(t)    
\end{align*}

With explicit solution:
\begin{align}
\label{eq: r_VAS_explicit}
r(T) &= e^{-\alpha(T)}r(0) + m[1-e^{-\alpha(T)}] 
+ \sigma \int_{0}^{T}e^{-\alpha(T-u)}dW^{Q}(u) \nonumber \\ 
&= 
m + e^{-\alpha T}\left[r(0) -m \right]
+ \sigma \int_{0}^{T}e^{-\alpha(T-u)}dW^{Q}(u)
\end{align} 


\begin{proposition}[\textbf{\cite{remillard2013statistical}}]
One can express the Vasicek model as an $AR(1)$-process: 
\begin{align*}
r_{k} &= m + \phi (r_{k-1}-m) + \epsilon_{i}, \; k = 1, \dots, n     
\end{align*}
Where:
\begin{itemize}[leftmargin =*]
    \item $r_{i} := r(ih)$, here $r$ is as described in Equation \ref{eq: r_VAS_explicit}. 
    \item $h$ is an equidistant time-interval between $r_{i}$ and $r_{i-1}$. 
    \item $\phi = e^{-\alpha h}$
    \item $\epsilon_{i} \sim \mathcal{N}\left(
    0, \frac{\sigma^{2}}{2\alpha}\left[
    1-\phi^{2}
    \right]\right)$
\end{itemize}
\end{proposition}

\newpage 
\textbf{Maximum Likelihood Estimation}
\\~\\ 
We can find the MLE \nomenclature{MLE}{Maximum Likelihood Estimation}  estimates for $\bm{\theta} = (\alpha, m, \sigma)$. We note that our $AR(1)$-series $r = (r_{k})_{\{k = 0, 1, \dots, n\}}$ is Markovian, meaning that: 
\begin{align*}
r_{k}|r_{k-1}, \dots, r_{0} \stackrel{d}{=} r_{k}|r_{k-1}    
\end{align*}

This means that: 
\begin{align*}
L(\bm{\theta}) &= f_{\bm{\theta}}(r_{1}, \dots, r_{n}|r_{0}) \\ 
&= \prod_{k=1}^{n}f_{\bm{\theta}}(r_{k}|r_{k-1})
\end{align*}

We have that 
\[
r_{k}|r_{k-1} \sim \mathcal{N}\left(
\E[r_{k}|r_{k-1}], Var[r_{k}|r_{k-1}]
\right)
\]

as: 
\begin{align*}
r_{k} &= \underbrace{m + e^{-\alpha h}(r_{k-1}-m)}_{= \E[r_{k}|r_{k-1}]} 
+ \underbrace{\epsilon_{k}}_{\sim \mathcal{N}(0, \sigma_{w}^{2})}    
\end{align*}

Meaning that: 
\begin{align*}
r_{k}|r_{k-1} \sim \mathcal{N}\left(
\underbrace{
m + e^{-\alpha h}[r_{k-1}-m]
}_{= \mu}, 
\underbrace{
\frac{\sigma^{2}}{2\alpha}\left[1-e^{-2\alpha h}\right]
}_{= \sigma_{w}^{2}}
\right)    
\end{align*}

Now: 
\begin{align*}
f_{\mathcal{N}(\mu, \sigma_{w}^{2})}(r_{k}) &=
\frac{1}{\sqrt{2\pi}\sigma_{w}}\exp\left(
-\frac{1}{2\sigma_{w}^{2}}\left[r_{k} - \mu \right]^{2}
\right)
\end{align*}

This gives us the following: 
\begin{align*}
L(\bm{\theta}) &= \prod_{k=1}^{n}f_{\bm{\theta}}(r_{k}|r_{k-1}) \\ 
&= 
\prod_{k=1}^{n}f_{\mathcal{N}(\mu, \sigma_{w}^{2})}(r_{k}) 
\\ 
&= 
(2\pi\sigma_{w}^{2})^{-\frac{n}{2}}\exp\left(
-\frac{1}{2\sigma_{w}^{2}}\sum_{k=1}^{n}\left(
r_{k} - \mu
\right)^{2}
\right) \\ 
&\Downarrow \\ 
l(\bm{\theta}) &:= \ln L(\bm{\theta}) \\
&= 
-\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma_{w}^{2})
-\frac{1}{2\sigma_{w}^{2}}\sum_{k=1}^{n}\left(r_{k} - \mu \right)^{2}
\end{align*}

\newpage
From \cite{Chalmers_MSc}, we can infer the following estimates: 
\begin{align*}
\hat{\alpha} &= 
-\frac{1}{n}\ln\left[
\frac{
n\sum_{k=1}^{n}r_{k}r_{k-1} - \sum_{k=1}^{n}r_{k}\sum_{k=1}^{n}r_{k-1}
}{
n\sum_{k=1}^{n}r_{k-1}^{2} - \left(\sum_{k=1}^{n}r_{k-1}\right)^{2}
}
\right] \\ 
\\ 
\hat{m} &= 
\frac{1}{n[1-e^{\hat{\alpha}h}]}
\left[
\sum_{k=1}^{n}r_{k} - e^{-\hat{\alpha}h}\sum_{k=1}^{n}r_{k-1}
\right] \\ 
\\ 
\hat{\sigma}^{2} &= 
\frac{
2\hat{\alpha}
}
{
n[1-e^{-2\hat{\alpha}h}]
}
\sum_{k=1}^{n}\left(
r_{k} - r_{k-1}e^{-\hat{\alpha}h} 
- \hat{m}[1-e^{-\hat{\alpha}h}]
\right)^{2}
\end{align*} 







\newpage 
Our approach will be based on \cite{halga2014}. We have the following relationship: 
\begin{align*}
P(t,T) &= e^{-R(t,T)(T-t)}
\end{align*}
Here $R(t,T)$ is the zero coupon yield also called the yield term structure. 


We will study interest rates which are ATS, meaning that the zero-coupon bond can be written as:
\begin{align*}
P(t,T) &= 
\exp\left(
-A(t,T) -B(t,T)r(t)
\right)
\end{align*}

This gives us the following relationship between ATS and zero-coupon yields: 
\begin{align*}
R(t,T) &= \frac{
A(t,T) + B(t,T)r(t)
}{
T-t
}    
\end{align*}

Assume that there are $n$ observations, and $M$ maturities, we denote: 
\begin{itemize}
    \item $\tau_{j} := T_{j} - t_{i}$
    \item $R_{ij}$: the observed zero-coupon yield on day $i$ with maturity $j$. 
\end{itemize}

\textbf{Vasicek}
\\~\\
Our goal will be to minimize the following: 
\begin{align*}
\argmin\limits_{(\alpha, m, \sigma, r_{i}) \in \R^{4}}
S_{i}(\alpha, m, \sigma, r_{i}, \tau_{j}) 
&= 
\sum_{j=1}^{M}\left[
R_{ij} - R^{VAS}(\alpha, m, \sigma, r_{i}, \tau_{j})
\right]^{2}
\end{align*}

Here: 
\begin{align*}
R^{VAS}(\alpha, m, \sigma, r_{i}, \tau_{j})
&= 
\frac{
A^{VAS}(\alpha, m, \sigma, \tau_{j}) + B^{VAS}(\alpha, \tau_{j})r_{i}
}{
\tau_{j}
} 
\end{align*}

Where: 
\begin{align*}
B^{VAS}(\alpha, \tau_{j}) &= \frac{1}{\alpha}\left[
1 - e^{-\alpha \tau_{j}}
\right] \\ 
A^{VAS}(\alpha, m, \sigma, \tau_{j})
&= 
\left(
\frac{\sigma^{2}}{2\alpha^{2}} - m
\right)\left[
\tau_{j} - B^{VAS}(\alpha, \tau_{j})
\right]
+ \frac{\sigma^{2}}{4\alpha}B^{2}(\alpha, \tau_{j})
\end{align*}

\newpage 

s



\newpage 
For simulation purposes, we have collected zero-coupon yields from the Bank of Canada \cite{Canada_termyields}. We have collected data from 22/Mar/2013-08/Mar/2023: 
\\~\\ 
\textbf{Plot here}
\\~\\



