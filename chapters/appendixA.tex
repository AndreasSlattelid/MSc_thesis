\chapter{Estimating parameters for interest rate models} 

\begin{definition}[\textbf{ARMA(p,q)}]
A stochastic process $Y = (Y_{i})_{i\geq 1}$ is called an $ARMA(p,q)$ process, if it has the following representation: 
\begin{align*}
Y_{i} &= \mu + \sum_{j=1}^{p}\phi_{j}(Y_{i-j}-\mu) + \epsilon_{i} - \sum_{k=1}^{q}\theta_{k}\epsilon_{i-k} \end{align*}
Where: 
\begin{itemize}[leftmargin =*]
    \item $\epsilon_{i}$ are iid with $\E[\epsilon_{i}] = 0$ and $Var[\epsilon_{i}] = \sigma_{\epsilon}^{2}$ as well as independent of \\ 
    $\mathcal{Y}_{i-1} = (Y_{1}, \dots, Y_{i-1})$
\end{itemize}
\end{definition}


A special case of an $ARMA(p,q)$ is $AR(p)$, we have the the following relationship: 
\[
AR(p) = ARMA(p,0)
\]

We will be interested in $AR(1)$, meaning that: 
\[
Y_{i} = \mu + \phi[Y_{i-1}-\mu] + \epsilon_{i}
\]

\textbf{Maximum likelihood AR(1)}
\\
Consider the case when $\epsilon_{i} \sim \mathcal{N}(0, \sigma_{w}^{2})$, in order to find the MLE estimates: $\bm{\theta} = (\mu, \phi, \sigma_{w}^{2})$, one can use the conditional likelihood function $L(\bm{\theta}|Y_{1})$. We note that $Y_{i}|Y_{i-1}, i \geq 2$ is Markovian, meaning that the conditional likelihood takes the following form: 
\begin{align*}
L(\bm{\theta}|Y_{1}) &= \prod_{i=2}^{n}f_{Y_{i}|Y_{i-1}}(y_{i}|y_{i-1})  
\end{align*}

Furthermore $Y_{i}|Y_{i-1} \sim \mathcal{N}(\mu + \phi[Y_{i-1}-\mu], \sigma_{w}^{2})$, meaning that: 

\begin{align*}
L(\bm{\theta}|Y_{1}) &= \left(
\frac{1}{
\sqrt{
2\pi\sigma_{w}^{2}
}
}
\right)^{n-1}\prod_{i=2}^{n}\exp\left(
-\frac{1}{2\sigma_{w}^{2}}\left[
y_{i}-(\mu + \phi[y_{i-1}-\mu])
\right]^{2}
\right) \\ 
&\Downarrow \\ 
l(\bm{\theta}|Y_{1}) &= 
(n-1)\ln\left(
\frac{1}{
\sqrt{
2\pi\sigma_{w}^{2}
}
}
\right)
-\frac{1}{2\sigma_{w}^{2}}
\sum_{i=2}^{n}
\left[
y_{i}-\mu - \phi[y_{i-1}-\mu])
\right]^{2}
\end{align*}

Now from \cite{remillard2013statistical}, it follows that the MLE estimates are approximate: 
\begin{align*}
\hat{\mu} &= \frac{1}{n}\sum_{i=1}^{n}Y_{i} = \overline{Y} \\ 
\hat{\phi} &= \frac{
\sum_{i=2}^{n}(Y_{i}-\overline{Y})(Y_{i-1}-\overline{Y})
}{
\sum_{i=1}^{n}(Y_{i}-\overline{Y})^{2}
} \\ 
\hat{\sigma}^{2}_{w} &= 
\frac{1}{n-1}\sum_{i=2}^{n}[
Y_{i-1}-\overline{Y} -\hat{\phi}(Y_{i-1}-\overline{Y})
]^{2}
\end{align*}


Now recall that the Vasicek model looks like:
\begin{align*}
dr(t) &= \alpha[m - r(t)]dt + \sigma dW^{Q}(t)    
\end{align*}

With explicit solution:
\begin{align}
\label{eq: r_VAS_explicit}
r(T) &= e^{-\alpha(T)}r(0) + m[1-e^{-\alpha(T)}] 
+ \sigma \int_{0}^{T}e^{-\alpha(T-u)}dW^{Q}(u) \nonumber \\ 
&= 
m + e^{-\alpha T}\left[r(0) -m \right]
+ \sigma \int_{0}^{T}e^{-\alpha(T-u)}dW^{Q}(u)
\end{align} 


\begin{proposition}[\textbf{\cite{remillard2013statistical}}]
One can express the Vasicek model as an $AR(1)$-process: 
\begin{align}
\label{eq: r_AR(1)}
r_{k} &= m + \phi (r_{k-1}-m) + \epsilon_{k}, \; k = 1, \dots, n     
\end{align}
Where:
\begin{itemize}[leftmargin =*]
    \item $r_{k} := r(kh)$, here $r$ is as described in Equation \ref{eq: r_VAS_explicit}. 
    \item $h$ is an equidistant time-interval between $r_{i}$ and $r_{i-1}$. 
    \item $\phi = e^{-\alpha h}$
    \item $\epsilon_{k} \sim \mathcal{N}\left(
    0, \frac{\sigma^{2}}{2\alpha}\left[
    1-\phi^{2}
    \right]\right)$
\end{itemize}
\end{proposition}

Thus in order to estimate $m, \phi$ and $\sigma_{w}^{2} = 
\frac{\sigma^{2}}{2\alpha}\left[
    1-\phi^{2}
    \right]
$ from Equation \ref{eq: r_AR(1)} one can plug it into the approximate MLE $AR(1)$-estimates. 
\\~\\ 
There are also other expressions for the MLE estimates like the following from \cite{fergusson2015application}: 
\begin{align*}
\hat{m} &= \frac{
S_{1}S_{00}-S_{0}S_{01}
}{
S_{0}S_{1}-S_{0}^{2}-S_{01} + S_{00}
}\\
\\ 
\hat{\alpha} &= \frac{1}{h}\ln\left(
\frac{S_{0}-\hat{m}}{S_{1}-\hat{m}}
\right) \\ 
\\ 
\hat{\sigma}^{2} &= 
\frac{
1}{
n\beta(\hat{\alpha})[1-\frac{1}{2}\hat{\alpha}\beta(\hat{\alpha})]
}
\sum_{k=1}^{n}[
r_{k}-\ell_{k-1}(k)
]^{2}
\end{align*}

\newpage 

Where:
\begin{align*}
S_{0} &= \frac{1}{n}\sum_{k=1}^{n}r_{k}, \;\; 
S_{1} = \frac{1}{n}\sum_{k=1}^{n}r_{k-1} \\ 
S_{00} &= \frac{1}{n}\sum_{k=1}^{n}r_{k-1}r_{k-1}, \;\; 
S_{01} = \frac{1}{n}\sum_{k=1}^{n}r_{k-1}r_{k}
\end{align*}

Furthermore: 
\begin{itemize}[leftmargin =*]
    \item $\beta(\alpha) =\frac{1}{\alpha}\left[
    1 - e^{-\alpha h}
    \right]$ 
    \item $\ell_{s}(t) = m\cdot\alpha\cdot B(s,t) + r_{s}[1-\alpha B(s,t)] $ 
    \item $B(s,t) = \frac{1}{\alpha}\left[
    1-e^{-\alpha(t-s)}\right]$
\end{itemize}


Typically we will not observe the short-rate $r$. However, zero coupon bond yields are observable, often called the yield term structure. The idea is to establish the connection between the zero coupon yields and Affine term structures:

\begin{align*}
P(t,T) &= e^{-R(t,T)(T-t)}, \;\text{and}\;
P(t,T) = \exp\left(
-A(t,T) -B(t,T)r(t)
\right)
\end{align*}

This gives us the following relationship between ATS and zero-coupon yields: 
\begin{align*}
R(t,T) &= \frac{
A(t,T) + B(t,T)r(t)
}{
T-t
}    
\end{align*}

Let $\tau = T-t$; we can then express the short rate. For the Vasicek model, we have the following: 
\begin{align*}
B_{\tau}(\bm{\theta}) &= \frac{1}{\alpha}[1-e^{-\alpha \tau}] \\ 
A_{\tau}(\bm{\theta}) &= 
\left(
\frac{\sigma^{2}}{2\alpha} - m
\right)[\tau - B_{\tau}(\bm{\theta})]
+ \frac{\sigma^{2}}{4\alpha}B_{\tau}^{2}(\bm{\theta})
\end{align*}

This again gives us the following expression for the short-rate: 
\begin{align*}
r(t) := r_{\bm{\theta}}(t) 
&= 
\frac{
\tau R(t,t+\tau) - A_{\tau}(\bm{\theta})
}{
B_{\tau}(\bm{\theta})
}
\end{align*}

Let $R_{1}, \dots, R_{n}$ represent the observed annualized zero coupon yields with maturities $\tau_{1}, \dots, \tau_{n}$, meaning that: 
\begin{align*}
R_{k} := R(kh, kh + \tau_{k})    
\end{align*}
This yields: 
\begin{align*}
r_{k}:= r_{k}^{\bm{\theta}}
&= 
\frac{
\tau_{k}R_{k}-A_{\tau_{k}}(\bm{\theta})
}{
B_{\tau_{k}}(\bm{\theta})
}
\end{align*}

We have that $\bm{R} = (R_{1}, \dots, R_{n})$ are observable, while $\bm{r} = (r_{1}, \dots, r_{n})$ are not. We note that: 
\begin{align*}
R_{k} &= \frac{
A_{\tau_{k}}(\bm{\theta}) + B_{\tau_{k}}(\bm{\theta})r_{k}
}{
\tau_{k}
}
:= g(r_{k})
\end{align*}

Here $g$ is a linear function of $r_{k}$, meaning it is invertible and strictly increasing. This means that: 
\begin{align*}
 f_{\bm{\theta}}(\bm{R}|R_{1}) &= \frac{
 f_{\bm{\theta}}(\bm{r}|r_{1})
 }{
 |J_{g}(\bm{r})|
 }   
\end{align*}

Here: 
\begin{align*}
J_{g}(\bm{r}) &= \det\left(
\frac{\partial g_{i}(\bm{r})}{
\partial r_{k}
}
\right)_{i,k = 1}^{n}    
\end{align*}

In our case we have $g_{i} = g$, with: 
\begin{align*}
\frac{
\partial g(\bm{r})
}{
\partial r_{l}
}
&= 
\begin{cases}
      \frac{B_{\tau_{k}(\bm{\theta})}}{\tau_{k}} = \frac{1}{a\tau_{k}}[1-e^{-a\tau_{k}}] \neq 0
      &, \; l = k\\
      0 &, \; l \neq k
    \end{cases}  
\end{align*}

This means that $J_{g}$ is a diagonal matrix, giving us the following determinant: 
\begin{align*}
|J_{g}(\bm{r})| 
&= 
\prod_{k=1}^{n}
\bigg{|}
\frac{\partial g(\bm{r})}{\partial r_{k}}
\bigg{|}
= 
\prod_{k=1}^{n}\frac{B_{\tau_{k}(\bm{\theta})}}{\tau_{k}}
\end{align*}

And again from \cite{remillard2013statistical} we get that:
\begin{align*}
l(\bm{\theta}|R_{1}) &= 
\ln\left[
\frac{
 f_{\bm{\theta}}(\bm{r}|r_{1})
 }{
 |J_{g}(\bm{r})|
 }  
\right] \\
&= 
l(\bm{\theta}|g^{-1}(R_{1})) - \ln(|J_{g}(\bm{r})|) \\ 
&= 
(n-1)\ln\left(
\frac{1}{
\sqrt{2\pi \sigma_{\epsilon}^{2}}
}
\right)
- \frac{1}{2\sigma_{\epsilon}^{2}}
\sum_{k=2}^{n}\left[
r_{k}^{\bm{\theta}} - m - \phi(r_{k-1}^{\bm{\theta}} -m)
\right]^{2}
- \sum_{k=1}^{n}\frac{B_{\tau_{k}(\bm{\theta})}}{\tau_{k}}
\end{align*}









